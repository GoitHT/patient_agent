import os
import sys
import operator
import re
import logging
from typing import List, TypedDict, Annotated

# --- æ™ºèƒ½æ¨¡å‹ä¸‹è½½ï¼šæ£€æŸ¥æœ¬åœ°æ˜¯å¦æœ‰æ¨¡å‹ï¼Œæ²¡æœ‰åˆ™ä¸´æ—¶å…è®¸ä¸‹è½½ ---
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)
CACHE_FOLDER = os.path.join(ROOT_DIR, "model_cache")
EMBED_MODEL_NAME = "BAAI/bge-large-zh-v1.5"

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
model_cache_path = os.path.join(CACHE_FOLDER, "models--BAAI--bge-large-zh-v1.5")
model_exists = os.path.exists(model_cache_path) and os.path.isdir(model_cache_path)

if not model_exists:
    print(f"âš ï¸  æœªæ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ç¼“å­˜: {model_cache_path}")
    print("ğŸ“¥ é¦–æ¬¡è¿è¡Œï¼Œå°†åœ¨çº¿ä¸‹è½½æ¨¡å‹...")
    # ä¸´æ—¶å…è®¸ä¸‹è½½
    os.environ['HF_HUB_OFFLINE'] = '0'
    os.environ['TRANSFORMERS_OFFLINE'] = '0'
else:
    print(f"âœ… æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ç¼“å­˜ï¼Œä½¿ç”¨ç¦»çº¿æ¨¡å¼")
    # å¼ºåˆ¶ä½¿ç”¨ç¦»çº¿æ¨¡å¼
    os.environ['HF_HUB_OFFLINE'] = '1'
    os.environ['TRANSFORMERS_OFFLINE'] = '1'

os.environ['HF_HOME'] = CACHE_FOLDER  # æŒ‡å®šHuggingFaceç¼“å­˜ç›®å½•

# ä¿®å¤ï¼šå¯¼å…¥æ­£ç¡®çš„Chromaç‰ˆæœ¬
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage
from langchain_core.tools import tool
from langgraph.graph import StateGraph, END
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate

# --- 1. å…¨å±€é…ç½®ï¼ˆä¸å˜ï¼‰ ---
DEEPSEEK_KEY = 'sk-16ecbb2a436c410e870b3ec10c87a84b'
DEEPSEEK_BASE = 'https://api.deepseek.com'
COSINE_DISTANCE_THRESHOLD = 0.3  # ä¸´æ—¶æ”¾å®½ï¼Œç¡®ä¿èƒ½æ£€ç´¢åˆ°
# EMBED_MODEL_NAME å·²åœ¨ä¸Šé¢å®šä¹‰


# --- 2. åµŒå…¥æ¨¡å‹åˆå§‹åŒ–ï¼ˆæ”¯æŒè‡ªåŠ¨ä¸‹è½½ï¼‰ ---
def init_embeddings():
    """ä»æœ¬åœ°ç¼“å­˜åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼Œé¦–æ¬¡è¿è¡Œè‡ªåŠ¨ä¸‹è½½"""

    print(f"ğŸ“‚ åµŒå…¥æ¨¡å‹ç¼“å­˜è·¯å¾„: {CACHE_FOLDER}")
    print(f"ğŸ“‚ ç¼“å­˜è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.exists(CACHE_FOLDER)}")

    # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
    if os.path.exists(CACHE_FOLDER):
        import glob
        model_files = glob.glob(os.path.join(CACHE_FOLDER, "**/*.bin"), recursive=True) + \
                      glob.glob(os.path.join(CACHE_FOLDER, "**/*.safetensors"), recursive=True)
        print(f"ğŸ“‚ æ‰¾åˆ° {len(model_files)} ä¸ªæ¨¡å‹æ–‡ä»¶")

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBED_MODEL_NAME,
            model_kwargs={"device": "cpu"},
            encode_kwargs={
                "normalize_embeddings": True,
                "batch_size": 32
            },
            cache_folder=CACHE_FOLDER
        )
        if model_exists:
            print("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼‰")
        else:
            print("âœ… åµŒå…¥æ¨¡å‹ä¸‹è½½å¹¶åˆå§‹åŒ–æˆåŠŸ")
            # ä¸‹è½½å®Œæˆåï¼Œé‡æ–°è®¾ç½®ä¸ºç¦»çº¿æ¨¡å¼
            os.environ['HF_HUB_OFFLINE'] = '1'
            os.environ['TRANSFORMERS_OFFLINE'] = '1'

        # æµ‹è¯•åµŒå…¥æ¨¡å‹æ˜¯å¦æ­£å¸¸å·¥ä½œ
        test_emb = embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
        print(f"ğŸ”¬ åµŒå…¥å‘é‡æµ‹è¯•: ç»´åº¦={len(test_emb)}, èŒƒæ•°={sum(x * x for x in test_emb) ** 0.5:.4f}")

        return embeddings
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")

        # å¤‡é€‰æ–¹æ¡ˆï¼šå°è¯•ç›´æ¥åŠ è½½sentence-transformers
        try:
            print("ğŸ”„ å°è¯•å¤‡é€‰åŠ è½½æ–¹æ¡ˆ...")
            from sentence_transformers import SentenceTransformer

            # ç›´æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
            model_path = os.path.join(CACHE_FOLDER, "models--BAAI--bge-large-zh-v1.5")
            if not os.path.exists(model_path):
                model_path = EMBED_MODEL_NAME

            model = SentenceTransformer(
                model_path,
                cache_folder=CACHE_FOLDER,
                device='cpu'
            )

            # åŒ…è£…æˆLangChainå…¼å®¹çš„åµŒå…¥æ¨¡å‹
            class LocalEmbeddings(HuggingFaceEmbeddings):
                def __init__(self, model):
                    self.model = model
                    super().__init__()

                def embed_documents(self, texts):
                    return self.model.encode(texts, normalize_embeddings=True, batch_size=32)

                def embed_query(self, text):
                    return self.model.encode([text], normalize_embeddings=True, batch_size=32)[0]

            embeddings = LocalEmbeddings(model)
            print("âœ… å¤‡é€‰åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ")
            return embeddings

        except Exception as e2:
            print(f"âŒ å¤‡é€‰æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e2}")

            # è¿”å›ä¸€ä¸ªç®€å•çš„å ä½ç¬¦åµŒå…¥æ¨¡å‹ï¼ˆä»…ç”¨äºæµ‹è¯•ï¼‰
            class DummyEmbeddings:
                def embed_documents(self, texts):
                    return [[0.1] * 768 for _ in texts]

                def embed_query(self, text):
                    return [0.1] * 768

            print("âš ï¸ ä½¿ç”¨è™šæ‹ŸåµŒå…¥æ¨¡å‹ï¼ˆåŠŸèƒ½å—é™ï¼‰")
            return DummyEmbeddings()


embeddings = init_embeddings()


# --- 3. æ ¸å¿ƒä¿®å¤ï¼šåŠ¨æ€åŠ è½½å‘é‡åº“ï¼ˆå…³é”®ï¼æ¯æ¬¡æ£€ç´¢éƒ½é‡æ–°è¯»å–æœ€æ–°æ•°æ®ï¼‰ ---
def get_high_quality_qa_db():
    """
    åŠ¨æ€åŠ è½½é«˜è´¨é‡é—®ç­”å‘é‡åº“
    è§£å†³ï¼šæ¨¡å—å¯åŠ¨æ—¶åˆå§‹åŒ–ã€æ›´æ–°åæ— æ³•è¯»å–æ–°æ•°æ®çš„é—®é¢˜
    """
    # ç²¾å‡†è®¡ç®—è·¯å¾„ï¼ˆé€‚é…ä½ çš„æ–‡ä»¶ç»“æ„ï¼šSPLLM-RAG1y/chroma/HighQualityQA_dbï¼‰
    # Adaptive_RAG.py åœ¨ state/ ä¸‹ï¼Œæ‰€ä»¥æ ¹ç›®å½•æ˜¯ state/ çš„ä¸Šä¸€çº§
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(CURRENT_DIR)  # æ­£ç¡®æŒ‡å‘ SPLLM-RAG1y/
    DB_PATH = os.path.join(ROOT_DIR, "chroma", "HighQualityQA_db")

    # è°ƒè¯•ï¼šæ‰“å°å®é™…è·¯å¾„ï¼Œç¡®è®¤æ˜¯å¦æ­£ç¡®
    print(f"[åŠ¨æ€åŠ è½½] HighQualityQA_db è·¯å¾„: {DB_PATH}")
    print(f"[åŠ¨æ€åŠ è½½] è·¯å¾„æ˜¯å¦å­˜åœ¨: {os.path.exists(DB_PATH)}")

    # åŠ¨æ€åˆ›å»º/åŠ è½½å‘é‡åº“ï¼ˆå¼ºåˆ¶ä½™å¼¦è·ç¦»ï¼‰
    db = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embeddings,
        collection_name="HighQualityQA",
        collection_metadata={"hnsw:space": "cosine"}
    )

    # è°ƒè¯•ï¼šæ‰“å°å‘é‡åº“å†…å®é™…æ–‡æ¡£æ•°
    doc_count = db._collection.count()
    print(f"[åŠ¨æ€åŠ è½½] å‘é‡åº“å†…æ–‡æ¡£æ€»æ•°: {doc_count}")
    return db


# --- å·¥å…·å‡½æ•°ï¼ˆä¸å˜ï¼‰ ---
@tool
def retrieve_docs(query: str):
    """å½“ç”¨æˆ·è¯¢é—®åŒ»å­¦ä¸“ä¸šé—®é¢˜ï¼Œæˆ–éœ€è¦è”ç³»æ‚£è€…ä¹‹å‰çš„æ²Ÿé€šå†å²æ—¶ï¼Œè°ƒç”¨æ­¤å·¥å…·ã€‚"""
    return "å·²è§¦å‘æ£€ç´¢æµç¨‹"


# --- çŠ¶æ€å®šä¹‰ï¼ˆä¸å˜ï¼‰ ---
class GraphState(TypedDict):
    patient_id: str
    question: str
    messages: Annotated[List[BaseMessage], operator.add]
    generation: str
    documents: List[str]
    history_context: str
    score: float
    evaluation_result: dict


# --- 4. æ ¸å¿ƒä¿®å¤ï¼šretrieve_high_quality_qa å‡½æ•°ï¼ˆå½»åº•é‡æ„ï¼‰ ---
def retrieve_high_quality_qa(query: str, top_k: int = 3) -> str:
    """
    æ£€ç´¢é«˜è´¨é‡é—®ç­”å‘é‡åº“ï¼ˆä¿®å¤ç‰ˆï¼‰
    æ ¸å¿ƒæ”¹è¿›ï¼šåŠ¨æ€åŠ è½½å‘é‡åº“ + å…ˆè°ƒè¯•ååˆ¤æ–­ + ä½œç”¨åŸŸä¿®å¤
    """
    # ç¬¬ä¸€æ­¥ï¼šå…ˆæ ¡éªŒæŸ¥è¯¢ï¼ˆæ’é™¤ç©ºæŸ¥è¯¢ï¼‰
    if not query or query.strip() == "":
        print("[QAåŒ¹é…è°ƒè¯•] æ— æŸ¥è¯¢å†…å®¹")
        return ""

    # ç¬¬äºŒæ­¥ï¼šåŠ¨æ€åŠ è½½æœ€æ–°çš„å‘é‡åº“ï¼ˆå…³é”®ï¼ï¼‰
    try:
        high_quality_qa_db = get_high_quality_qa_db()
    except Exception as e:
        print(f"[QAåŒ¹é…è°ƒè¯•] å‘é‡åº“åŠ è½½å¤±è´¥: {str(e)}")
        return ""

    # ç¬¬ä¸‰æ­¥ï¼šæ ¡éªŒå‘é‡åº“æ˜¯å¦æœ‰æ•°æ®
    doc_count = high_quality_qa_db._collection.count()
    if doc_count == 0:
        print(f"[QAåŒ¹é…è°ƒè¯•] å‘é‡åº“å†…æ— æ•°æ®ï¼ˆæ€»æ•°ï¼š{doc_count}ï¼‰")
        return ""

    # ç¬¬å››æ­¥ï¼šæ‰§è¡Œæ£€ç´¢ï¼ˆä½™å¼¦è·ç¦»ï¼‰
    try:
        results = high_quality_qa_db.similarity_search_with_score(query, k=top_k)
    except Exception as e:
        print(f"[QAåŒ¹é…è°ƒè¯•] æ£€ç´¢å¤±è´¥: {str(e)}")
        return ""

    if not results:
        print(f"[QAåŒ¹é…è°ƒè¯•] æ— æ£€ç´¢ç»“æœ")
        return ""

    # ç¬¬äº”æ­¥ï¼šè¿‡æ»¤å¹¶æ ¼å¼åŒ–ç»“æœ
    relevant_qa = []
    for doc, distance in results:
        print(f"[QAåŒ¹é…è°ƒè¯•] ä½™å¼¦è·ç¦»={distance:.4f}, æ–‡æ¡£å†…å®¹: {doc.page_content[:100]}...")

        if distance < COSINE_DISTANCE_THRESHOLD:
            similarity = max(0, (1 - distance / 2) * 100)
            question = doc.metadata.get("question", "")
            answer = doc.metadata.get("answer", "")
            relevant_qa.append(
                f"é—®é¢˜ï¼šã€Œ{question}ã€\n"
                f"ç­”æ¡ˆï¼šã€Œ{answer[:300]}...ã€\n"
                f"(ç›¸ä¼¼åº¦ï¼š{similarity:.1f}%)"
            )

    if relevant_qa:
        print(f"--- [é«˜è´¨é‡é—®ç­”åŒ¹é…] æ‰¾åˆ° {len(relevant_qa)} æ¡é«˜ç›¸å…³ä¼˜è´¨é—®ç­” ---")
        return "\n\n".join(relevant_qa)
    else:
        min_distance = min(d for _, d in results)
        print(f"--- [é«˜è´¨é‡é—®ç­”åŒ¹é…] æœ€å°è·ç¦» {min_distance:.4f}ï¼Œé«˜äºé˜ˆå€¼ {COSINE_DISTANCE_THRESHOLD} ---")
        return ""


# --- 5. èŠ‚ç‚¹å‡½æ•°ï¼ˆä»…ä¿®æ”¹ agent_node ä¸­çš„æ£€ç´¢è°ƒç”¨ï¼Œå…¶ä½™ä¸å˜ï¼‰ ---
def agent_node(state: GraphState):
    print("--- [è®°å¿†æ£€ç´¢] æ­£åœ¨è°ƒå–å†å²è®°å¿†... ---")
    p_id = state.get("patient_id", "default")
    query = state.get("question", "").strip()  # å»é™¤é¦–å°¾ç©ºæ ¼

    # åŠ è½½å†å²è®°å¿†ï¼ˆä¸å˜ï¼‰
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(CURRENT_DIR)
    history_db = Chroma(
        persist_directory=os.path.join(ROOT_DIR, "chroma", "UserHistory_db"),
        embedding_function=embeddings
    )
    related_memories = history_db.similarity_search(query, k=2, filter={"patient_id": p_id})
    history_text = "\n".join([d.page_content for d in related_memories]) if related_memories else "å°šæ— ç›¸å…³å†å²è®°å½•"
    print(f"--- [æ£€ç´¢ç»“æœ] å·²è°ƒå–æ‚£è€… {p_id} çš„ä¸“å±å†å² ---")

    # æ ¸å¿ƒä¿®æ”¹ï¼šè°ƒç”¨ä¿®å¤åçš„æ£€ç´¢å‡½æ•°ï¼ˆåŠ¨æ€åŠ è½½å‘é‡åº“ï¼‰
    high_quality_qa_text = retrieve_high_quality_qa(query)

    # åç»­ Prompt é€»è¾‘ï¼ˆä¸å˜ï¼‰
    prompt_path = os.path.join(CURRENT_DIR, "prompt", "Node1_Triage.txt")
    with open(prompt_path, 'r', encoding='utf-8') as f:
        SYSTEM_PROMPT = f.read()

    prompt_parts = [SYSTEM_PROMPT, f"ã€å½“å‰æ‚£è€…å†å²è®°å½•ã€‘ï¼š\n{history_text}"]
    if high_quality_qa_text:
        prompt_parts.append(f"ã€å…¨é‡é«˜ç›¸å…³ä¼˜è´¨é—®ç­”å‚è€ƒã€‘ï¼š\n{high_quality_qa_text}")
    full_system_prompt = "\n\n".join(prompt_parts)

    llm = ChatOpenAI(model='deepseek-chat', openai_api_key=DEEPSEEK_KEY, openai_api_base=DEEPSEEK_BASE, temperature=0)
    llm_with_tools = llm.bind_tools([retrieve_docs])
    prompt = ChatPromptTemplate.from_messages([("system", full_system_prompt), ("user", "{input}")])
    chain = prompt | llm_with_tools
    response = chain.invoke({"input": query})
    return {"messages": [response], "history_context": history_text, "question": query}


# --- å…¶ä½™èŠ‚ç‚¹å‡½æ•°ï¼ˆretrieve_node/generate_node/direct_answer_node/evaluation_node/record_memory_nodeï¼‰ä¿æŒä¸å˜ ---
def retrieve_node(state: GraphState):
    import concurrent.futures
    from functools import lru_cache
    print("--- [èŠ‚ç‚¹ 2] ç»“åˆå†å²è®°å¿†æ‰§è¡Œæ£€ç´¢ä¸å¤šç»´æç‚¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ---")
    question = f"èƒŒæ™¯ï¼š{state['history_context']} é—®é¢˜ï¼š{state['question']}"
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(CURRENT_DIR)

    # ========== æ–°å¢ï¼šå®šä¹‰åˆ†éš”ç¬¦å˜é‡ï¼Œé¿å…f-stringå†…ç›´æ¥ä½¿ç”¨åæ–œæ  ==========
    DOC_SEPARATOR = "\n---\n"  # æ–‡æ¡£ç‰‡æ®µåˆ†éš”ç¬¦ï¼Œæå‰å®šä¹‰

    # ========== ä¼˜åŒ–1ï¼šç¼“å­˜å‘é‡åº“å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½ ==========
    @lru_cache(maxsize=2)
    def get_vector_db(db_name):
        """ç¼“å­˜å‘é‡åº“å®ä¾‹ï¼Œé‡å¤è°ƒç”¨æ—¶ç›´æ¥å¤ç”¨"""
        db_path = os.path.join(ROOT_DIR, "chroma", db_name)
        return Chroma(
            persist_directory=db_path,
            embedding_function=embeddings,
            collection_metadata={"hnsw:space": "cosine"}  # ç¡®ä¿ä½™å¼¦è·ç¦»
        )

    # ========== ä¼˜åŒ–2ï¼šå¹¶è¡Œæ£€ç´¢ä¸¤ä¸ªå‘é‡åº“ï¼ˆæ›¿ä»£ä¸²è¡Œï¼‰ ==========
    def retrieve_single_db(db_name, query, k=3):  # ä¼˜åŒ–3ï¼šé™ä½kå€¼ï¼ˆä»5â†’3ï¼‰ï¼Œå‡å°‘å¤„ç†é‡
        db = get_vector_db(db_name)
        docs = db.similarity_search(query, k=k)
        # ä¼˜åŒ–4ï¼šæå‰è¿‡æ»¤ç©ºæ–‡æ¡£/æ— æ•ˆæ–‡æ¡£
        valid_docs = [d for d in docs if d.page_content.strip() and len(d.page_content) > 20]
        return valid_docs

    # å¹¶è¡Œæ‰§è¡ŒæŒ‡å—åº“+æ¡ˆä¾‹åº“æ£€ç´¢
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        guide_future = executor.submit(retrieve_single_db, "MedicalGuide_db", question)
        case_future = executor.submit(retrieve_single_db, "ClinicalCase_db", question)
        guide_docs = guide_future.result()
        case_docs = case_future.result()

    # æ— æœ‰æ•ˆæ–‡æ¡£æ—¶å¿«é€Ÿè¿”å›
    if not guide_docs and not case_docs:
        return {
            "documents": ["æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™"],
            "messages": [ToolMessage(
                tool_call_id=state["messages"][-1].tool_calls[0]["id"],
                content="æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™"
            )]
        }

    # ========== ä¼˜åŒ–5ï¼šæ‰¹é‡å¤„ç†æ–‡æ¡£ï¼Œå‡å°‘LLMè°ƒç”¨æ¬¡æ•° ==========
    llm = ChatOpenAI(
        model='deepseek-chat',
        openai_api_key=DEEPSEEK_KEY,
        openai_api_base=DEEPSEEK_BASE,
        temperature=0,
        request_timeout=15  # ä¼˜åŒ–6ï¼šç¼©çŸ­è¶…æ—¶æ—¶é—´ï¼Œé¿å…å¡é¡¿
    )
    refined_context = {"guides": [], "cases": [], "conflicts": []}

    # æ‰¹é‡å¤„ç†æŒ‡å—æ–‡æ¡£ï¼ˆâœ… ä¿®å¤ï¼šä½¿ç”¨é¢„å®šä¹‰åˆ†éš”ç¬¦å˜é‡ï¼‰
    if guide_docs:
        # ä¼˜åŒ–7ï¼šç²¾ç®€æ–‡æ¡£å†…å®¹ï¼ˆæˆªå–å‰1000å­—ç¬¦ï¼‰ï¼Œå‡å°‘LLMè¾“å…¥é•¿åº¦
        guide_texts = [f"æŒ‡å—ç‰‡æ®µ{i + 1}ï¼š{d.page_content[:1000]}" for i, d in enumerate(guide_docs)]
        # æ›¿æ¢ç›´æ¥çš„"\n---\n"ä¸ºé¢„å®šä¹‰å˜é‡ï¼Œæ¶ˆé™¤f-stringå†…çš„åæ–œæ 
        guide_prompt = f"""æ‰¹é‡æå–ä»¥ä¸‹åŒ»å­¦æŒ‡å—çš„æ ¸å¿ƒä¿¡æ¯ï¼ˆæ¯æ¡ä»…ä¿ç•™ï¼šæ ¸å¿ƒæ¨èã€é’ˆå¯¹ç—‡çŠ¶ã€æ¥æºï¼‰ï¼š
{DOC_SEPARATOR.join(guide_texts)}"""
        guide_response = llm.invoke(guide_prompt).content
        refined_context["guides"] = [guide_response]  # æ‰¹é‡è¾“å‡ºï¼Œæ›¿ä»£é€ä¸ªè°ƒç”¨

    # æ‰¹é‡å¤„ç†æ¡ˆä¾‹æ–‡æ¡£ï¼ˆâœ… ä¿®å¤ï¼šä½¿ç”¨é¢„å®šä¹‰åˆ†éš”ç¬¦å˜é‡ï¼‰
    if case_docs:
        case_texts = [f"æ¡ˆä¾‹ç‰‡æ®µ{i + 1}ï¼š{d.page_content[:800]}" for i, d in enumerate(case_docs)]  # è¿›ä¸€æ­¥ç²¾ç®€
        # ä¿®æ­£ï¼šä½¿ç”¨é¢„å®šä¹‰åˆ†éš”ç¬¦å˜é‡ï¼Œå½»åº•é¿å…f-stringå†…çš„åæ–œæ 
        case_prompt = f"""æ‰¹é‡åˆ†æä»¥ä¸‹ä¸´åºŠæ¡ˆä¾‹å¯¹å½“å‰é—®é¢˜çš„å‚è€ƒä»·å€¼ï¼ˆé—®é¢˜ï¼š{question}ï¼‰ï¼Œæ¯æ¡å›ç­”åŒ…å«ï¼š
1. ç›¸ä¼¼ç—‡çŠ¶ 2. æ²»ç–—æ–¹æ¡ˆ 3. åŒ»ç”Ÿç»éªŒ/è­¦ç¤º
{DOC_SEPARATOR.join(case_texts)}"""
        case_response = llm.invoke(case_prompt).content
        refined_context["cases"] = [case_response]  # æ‰¹é‡è¾“å‡ºï¼Œæ›¿ä»£é€ä¸ªè°ƒç”¨

    # ========== ä¼˜åŒ–8ï¼šç²¾ç®€å†²çªåˆ†æPromptï¼Œæå‡æ•ˆç‡ ==========
    if refined_context["guides"] and refined_context["cases"]:
        conflict_prompt = f"""å¯¹æ¯”ä»¥ä¸‹æŒ‡å—ä¸æ¡ˆä¾‹ï¼Œä»…æŒ‡å‡ºæ¡ˆä¾‹ä¸­æœªè¢«æŒ‡å—è¦†ç›–çš„ä¸´åºŠå®æ“ç‚¹ï¼š
æŒ‡å—æ ¸å¿ƒï¼š{refined_context['guides'][0][:500]}
æ¡ˆä¾‹æ ¸å¿ƒï¼š{refined_context['cases'][0][:500]}"""
        conflict_res = llm.invoke(conflict_prompt).content
        refined_context["conflicts"] = conflict_res
    else:
        refined_context["conflicts"] = "æ— è¶³å¤Ÿæ•°æ®è¿›è¡Œå¯¹æ¯”"

    return {
        "documents": [str(refined_context)],
        "messages": [ToolMessage(
            tool_call_id=state["messages"][-1].tool_calls[0]["id"],
            content="åŒåº“è”åˆæ£€ç´¢å®Œæˆï¼ˆä¼˜åŒ–ç‰ˆï¼‰"
        )]
    }

def generate_node(state: GraphState):
    print("--- [èŠ‚ç‚¹ 3] åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­” ---")
    refined_info = state.get("documents", ["æ— å¯ç”¨å‚è€ƒèµ„æ–™"])[0]
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    prompt_path = os.path.join(CURRENT_DIR, "prompt", "Node3_Generate.txt")
    with open(prompt_path, 'r', encoding='utf-8') as f:
        SYSTEM_PROMPT = f.read()

    llm = ChatOpenAI(model='deepseek-chat', openai_api_key=DEEPSEEK_KEY, openai_api_base=DEEPSEEK_BASE, temperature=0)
    response = llm.invoke([
        ("system", f"{SYSTEM_PROMPT}\nå†å²è®°å¿†ï¼š{state['history_context']}"),
        ("user", f"é—®é¢˜ï¼š{state['question']}\nèµ„æ–™ï¼š{refined_info}")
    ])
    return {"generation": response.content}


def direct_answer_node(state: GraphState):
    print("--- [èŠ‚ç‚¹ 4] ç»¼åˆå›ç­”æ§åˆ¶ä¸­å¿ƒ ---")
    llm = ChatOpenAI(model='deepseek-chat', openai_api_key=DEEPSEEK_KEY, openai_api_base=DEEPSEEK_BASE, temperature=0.7)
    docs = state.get("documents", [])
    is_fallback = len(docs) > 0
    h_context = state.get("history_context", "æ— ç›¸å…³å†å²è®°å½•")

    if is_fallback:
        system_msg = (
            f"ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ»ç”Ÿã€‚å·²çŸ¥æ‚£è€…å†å²èƒŒæ™¯ï¼š{h_context}ã€‚\n"
            "è™½ç„¶åœ¨åŒ»å­¦åº“ä¸­æœªåŒ¹é…åˆ°å¯¹æ ‡æ¡ç›®ï¼Œä½†è¯·åŸºäºé€šç”¨åŒ»å­¦çŸ¥è¯†ç»™å‡ºå»ºè®®ã€‚"
            "æ³¨æ„ï¼š1. è¯­æ°”ä¸“ä¸šè°¨æ…ï¼›2. æç¤ºå»ºè®®ä»…ä¾›å‚è€ƒã€‚"
        )
    else:
        system_msg = (
            f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—åŠ©æ‰‹ã€‚å‚è€ƒæ‚£è€…ä¹‹å‰çš„æ²Ÿé€šè®°å½•ï¼š{h_context}ã€‚\n"
            "è¯·å‹å¥½åœ°å›ç­”ç”¨æˆ·ã€‚æ‰“æ‹›å‘¼è¯·ç¤¼è²Œå›åº”ï¼›å¸¸è¯†é—®é¢˜è¯·ç®€æ´æ˜äº†ã€‚"
        )

    response = llm.invoke([("system", system_msg), ("user", state["question"])])
    return {"generation": response.content}


def evaluation_node(state: GraphState):
    import json
    print("--- [èŠ‚ç‚¹ 5] è´¨é‡è¯„ä¼°ä¸­å¿ƒ ---")
    question = state.get("question", "")
    generation = state.get("generation", "")
    history_context = state.get("history_context", "æ— è®°å½•")
    answer_text = generation.content if (hasattr(generation, 'content') and generation.content) else str(generation)

    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    prompt_path = os.path.join(CURRENT_DIR, "prompt", "Node5_Evaluation.txt")
    try:
        with open(prompt_path, 'r', encoding='utf-8') as f:
            full_system_prompt = f.read()
    except FileNotFoundError:
        print("âŒ è¯„ä¼°é…ç½®æ–‡ä»¶Node5_Evaluation.txtæœªæ‰¾åˆ°")
        return {"score": 0.0, "evaluation_result": {"total_score": 0.0, "is_high_quality": False}}

    prompt = ChatPromptTemplate.from_messages([
        ("system", full_system_prompt),
        ("user", "ã€æ‚£è€…æé—®ã€‘ï¼š{question}\nã€å†å²èƒŒæ™¯ã€‘ï¼š{history_context}\nã€åŒ»ç”Ÿå›ç­”ã€‘ï¼š{answer_text}")
    ])

    llm = ChatOpenAI(
        model='deepseek-chat',
        openai_api_key=DEEPSEEK_KEY,
        openai_api_base=DEEPSEEK_BASE,
        temperature=0,
        request_timeout=30
    )
    chain = prompt | llm

    try:
        response = chain.invoke({"question": question, "history_context": history_context, "answer_text": answer_text})
        eval_content = response.content.strip()
        evaluation_result = json.loads(eval_content)

        if "total_score" not in evaluation_result:
            base_score = sum(evaluation_result.get("dimension_scores", {}).values())
            bonus = evaluation_result.get("bonus_points", 0.0)
            penalty = evaluation_result.get("penalty_points", 0.0)
            total_score = round(base_score + bonus - penalty, 1)
            evaluation_result["total_score"] = total_score

        dim_scores = evaluation_result.get("dimension_scores", {})
        total_score = evaluation_result.get("total_score", 0.0)
        has_medical_guide = any(
            key in answer_text for key in ["å‰‚é‡", "æ£€æŸ¥", "ç”¨è¯", "å°±åŒ»æ—¶æœº", "æ–¹æ¡ˆ", "ç–—ç¨‹", "å‡é‡"])
        has_fabricated_info = evaluation_result.get("penalty_points", 0.0) <= -1.4
        is_high_quality = (
                total_score >= 8.0 and
                dim_scores.get("åŒ»å­¦å‡†ç¡®æ€§", 0.0) >= 1.2 and
                dim_scores.get("å®‰å…¨åˆè§„æ€§", 0.0) >= 1.0 and
                has_medical_guide and
                not has_fabricated_info
        )

        evaluation_result["is_high_quality"] = is_high_quality
        evaluation_result["answer_text"] = answer_text
        print(f"â­ è¯„ä¼°ç»“æœ: æ€»åˆ† {evaluation_result['total_score']}, é«˜è´¨é‡å¯¹è¯: {is_high_quality}")
        print(f"   7ç»´åº¦åˆ†æ•°: {evaluation_result['dimension_scores']}")
        print(f"   åŠ åˆ†é¡¹: {evaluation_result['bonus_points']} | æ‰£åˆ†é¡¹: {evaluation_result['penalty_points']}")
        return {"score": total_score, "evaluation_result": evaluation_result}
    except json.JSONDecodeError:
        print(f"âŒ è¯„ä¼°ç»“æœJSONè§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”ï¼š{eval_content}")
        return {"score": 0.0, "evaluation_result": {"total_score": 0.0, "is_high_quality": False}}
    except Exception as e:
        print(f"âŒ è¯„ä¼°èŠ‚ç‚¹è°ƒç”¨å¤±è´¥: {str(e)}")
        return {"score": 3.0, "evaluation_result": {"total_score": 3.0, "is_high_quality": False}}


def record_memory_node(state: GraphState):
    import sys
    CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
    ROOT_DIR = os.path.dirname(CURRENT_DIR)
    sys.path.append(ROOT_DIR)

    try:
        from create_database_general import store_chat_history_rag, store_doctor_qa_evolution, init_high_quality_qa_db
    except ImportError as e:
        print(f"âŒ å¯¼å…¥è®°å¿†å­˜å‚¨æ¨¡å—å¤±è´¥: {str(e)}")
        return state

    print("--- [èŠ‚ç‚¹ 6] è®°å¿†æŒä¹…åŒ–ä¸è¿›åŒ–ä¸­å¿ƒ ---")
    p_id = state.get("patient_id", "default")
    question = state.get("question", "")
    generation = state.get("generation", "")
    useful_info = state.get("history_context", "")
    evaluation_result = state.get("evaluation_result", {})
    is_high_quality = evaluation_result.get("is_high_quality", False)
    total_score = evaluation_result.get("total_score", 0.0)
    answer_text = evaluation_result.get("answer_text",
                                        generation.content if hasattr(generation, 'content') else str(generation))

    # å­˜å‚¨å¯¹è¯å†å² + é«˜è´¨é‡é—®ç­”
    store_chat_history_rag(question, answer_text, p_id)
    store_doctor_qa_evolution(
        question=question,
        answer=answer_text,
        rag_info=useful_info,
        patient_id=p_id,
        score=total_score,
        is_high_quality=is_high_quality
    )

    # æ ¸å¿ƒæ–°å¢ï¼šæ›´æ–°å‘é‡åº“åï¼Œ**ç«‹å³é‡æ–°åˆå§‹åŒ–**ï¼ˆç¡®ä¿ä¸‹ä¸€æ¬¡æ£€ç´¢èƒ½è¯»åˆ°æœ€æ–°æ•°æ®ï¼‰
    init_high_quality_qa_db()
    print("âœ… é«˜è´¨é‡é—®ç­”å‘é‡åº“å·²åŒæ­¥æœ€æ–°æ•°æ®")
    return state


# --- å†³ç­–å‡½æ•°ï¼ˆä¸å˜ï¼‰ ---
def should_retrieve(state: GraphState):
    last_message = state["messages"][-1]
    return "retrieve" if last_message.tool_calls else "direct_response"


def decide_to_generate(state: GraphState):
    print("--- [å†³ç­–é—¸é—¨] è¯„ä¼°æ£€ç´¢ç›¸å…³æ€§ ---")
    docs = state.get("documents", [])
    if not docs or "æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™" in str(docs):
        print(">>> å†³ç­–ï¼šçŸ¥è¯†åº“å®Œå…¨æ— å…³ï¼Œåˆ‡æ¢è‡³å¤§æ¨¡å‹ç›´æ¥å›ç­”")
        return "fallback"

    llm = ChatOpenAI(model='deepseek-chat', openai_api_key=DEEPSEEK_KEY, openai_api_base=DEEPSEEK_BASE, temperature=0)
    check_prompt = f"åˆ¤å®šä»¥ä¸‹èµ„æ–™ä¸é—®é¢˜æ˜¯å¦ç›¸å…³ã€‚åªéœ€å›å¤ [åˆæ ¼] æˆ– [ä¸ç›¸å…³]ã€‚\né—®é¢˜ï¼š{state['question']}\nèµ„æ–™ï¼š{str(docs)[:500]}"
    verification = llm.invoke(check_prompt).content

    if "[ä¸ç›¸å…³]" in verification:
        print(">>> å†³ç­–ï¼šèµ„æ–™è´¨é‡ä¸ä½³ï¼Œåˆ‡æ¢è‡³å¤§æ¨¡å‹ç›´æ¥å›ç­”")
        return "fallback"
    return "generate"


# --- å›¾æ„å»ºä¸ç¼–è¯‘ï¼ˆä¸å˜ï¼‰ ---
workflow = StateGraph(GraphState)
workflow.add_node("agent", agent_node)
workflow.add_node("retrieve_node", retrieve_node)
workflow.add_node("generate_node", generate_node)
workflow.add_node("direct_answer_node", direct_answer_node)
workflow.add_node("record_memory", record_memory_node)
workflow.add_node("evaluate", evaluation_node)
workflow.set_entry_point("agent")
workflow.add_conditional_edges("agent", should_retrieve,
                               {"retrieve": "retrieve_node", "direct_response": "direct_answer_node"})
workflow.add_conditional_edges("retrieve_node", decide_to_generate,
                               {"generate": "generate_node", "fallback": "direct_answer_node"})
workflow.add_edge("generate_node", "evaluate")
workflow.add_edge("direct_answer_node", "evaluate")
workflow.add_edge("evaluate", "record_memory")
workflow.add_edge("record_memory", END)
app = workflow.compile()

# --- ä¸»å‡½æ•°ï¼ˆæµ‹è¯•ç”¨ï¼Œä¸å˜ï¼‰ ---
if __name__ == "__main__":
    test_input = {
        "patient_id": "test_001",
        "question": "é«˜è¡€å‹æ‚£è€…çªå‘å¤´ç—›ã€ä¸€ä¾§è‚¢ä½“æ— åŠ›å’Œè¨€è¯­å«ç³Šè¯¥æ€ä¹ˆåŠï¼Ÿ",
        "messages": [],
        "generation": "",
        "documents": [],
        "history_context": "",
        "score": 0.0,
        "evaluation_result": {}
    }
    print("\n=== ğŸš€ è¿è¡ŒAdaptive RAGæµ‹è¯• ===")
    result = app.invoke(test_input)
    print(f"\nğŸ“ æœ€ç»ˆå›ç­”ï¼š\n{result['generation']}")
    print(
        f"\nğŸ“Š è¯„ä¼°ç»“æœï¼š\næ€»åˆ†: {result['score']}, é«˜è´¨é‡å¯¹è¯: {result['evaluation_result'].get('is_high_quality', False)}")