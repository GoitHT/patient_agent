import os
import sys
import chardet
import json
import re
import csv
import numpy as np
import shutil
import logging
import argparse
import time
from pathlib import Path
from typing import List, Dict, Any

# --- å…³é”®ä¿®å¤ï¼šåœ¨å¯¼å…¥ä»»ä½•HuggingFaceåº“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡å’Œè·¯å¾„ ---
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ç»å¯¹è·¯å¾„
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.dirname(CURRENT_DIR)
sys.path.insert(0, ROOT_DIR)  # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
CACHE_FOLDER = os.path.join(CURRENT_DIR, "model_cache")
MODEL_NAME = "BAAI/bge-large-zh-v1.5"

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
model_cache_path = os.path.join(CACHE_FOLDER, "models--BAAI--bge-large-zh-v1.5")
model_exists = os.path.exists(model_cache_path) and os.path.isdir(model_cache_path)

if not model_exists:
    print(f"âš ï¸  æœªæ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹: {model_cache_path}")
    print("ğŸ“¥ å°†åœ¨çº¿ä¸‹è½½æ¨¡å‹ï¼ˆçº¦1.3GBï¼‰...")
    os.environ['HF_HUB_OFFLINE'] = '0'
    os.environ['TRANSFORMERS_OFFLINE'] = '0'
else:
    print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹: {model_cache_path}")
    os.environ['HF_HUB_OFFLINE'] = '1'
    os.environ['TRANSFORMERS_OFFLINE'] = '1'

# è®¾ç½®ç¼“å­˜è·¯å¾„
os.environ['HF_HOME'] = CACHE_FOLDER

# ç°åœ¨æ‰å¯¼å…¥HuggingFaceç›¸å…³åº“
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document

# å¯¼å…¥è¿›åº¦æ¡ï¼ˆç”¨äº rebuild æ¨¡å¼ï¼‰
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  tqdm æœªå®‰è£…ï¼Œrebuild æ¨¡å¼å°†æ— è¿›åº¦æ¡æ˜¾ç¤º")

# å¯¼å…¥åŠ¨æ€åˆ†å—å™¨ï¼ˆç”¨äº rebuild æ¨¡å¼ï¼‰
try:
    from src.rag.dynamic_chunker import DynamicChunker, ChunkConfig, ChunkStrategy
    DYNAMIC_CHUNKER_AVAILABLE = True
except ImportError:
    DYNAMIC_CHUNKER_AVAILABLE = False
    print("âš ï¸  DynamicChunker æœªæ‰¾åˆ°ï¼Œrebuild æ¨¡å¼å°†ä½¿ç”¨å›ºå®šåˆ†å—")

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def get_normalized_embeddings():
    """åˆ›å»ºå½’ä¸€åŒ–çš„åµŒå…¥æ¨¡å‹ï¼ˆæ”¯æŒè‡ªåŠ¨ä¸‹è½½ï¼‰"""
    embeddings = HuggingFaceEmbeddings(
        model_name=MODEL_NAME,
        model_kwargs={'device': 'cpu'},
        encode_kwargs={
            'normalize_embeddings': True,  # å¼ºåˆ¶å½’ä¸€åŒ–
            'batch_size': 32
        },
        cache_folder=CACHE_FOLDER
    )
    
    # ä¸‹è½½å®Œæˆåæ¢å¤ç¦»çº¿æ¨¡å¼
    if not model_exists:
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        print("âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼Œå·²åˆ‡æ¢å›ç¦»çº¿æ¨¡å¼")
    
    # éªŒè¯å½’ä¸€åŒ–æ•ˆæœ
    test_emb = embeddings.embed_query("æµ‹è¯•æ–‡æœ¬")
    norm = np.linalg.norm(test_emb)
    print(f"åµŒå…¥å‘é‡å½’ä¸€åŒ–éªŒè¯ï¼šèŒƒæ•° = {norm:.4f}ï¼ˆç†æƒ³å€¼=1.0ï¼‰")
    return embeddings


# å…¨å±€åµŒå…¥æ¨¡å‹ï¼ˆç¡®ä¿æ‰€æœ‰å‘é‡åº“ä½¿ç”¨åŒä¸€å½’ä¸€åŒ–æ¨¡å‹ï¼‰
EMBEDDINGS = get_normalized_embeddings()


# =============================================================================
# åŠ¨æ€åˆ†å—ç›¸å…³å‡½æ•°ï¼ˆç”¨äº rebuild æ¨¡å¼ï¼‰
# =============================================================================

def load_documents_from_json_rebuild(file_path: Path) -> List[Dict[str, Any]]:
    """ä» JSON æ–‡ä»¶åŠ è½½æ–‡æ¡£ï¼ˆrebuild æ¨¡å¼ä¸“ç”¨ï¼‰"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        logger.info(f"ğŸ“„ ä» {file_path.name} åŠ è½½äº† {len(data) if isinstance(data, list) else 1} ä¸ªæ–‡æ¡£")
        return data if isinstance(data, list) else [data]
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []


def load_documents_from_txt_rebuild(file_path: Path) -> List[Dict[str, Any]]:
    """ä» TXT æ–‡ä»¶åŠ è½½æ–‡æ¡£ï¼ˆrebuild æ¨¡å¼ä¸“ç”¨ï¼‰"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # ç®€å•åˆ†å‰²ï¼ˆæŒ‰åŒæ¢è¡Œç¬¦ï¼‰
        sections = content.split('\n\n')
        docs = []
        
        for i, section in enumerate(sections):
            if section.strip():
                docs.append({
                    "text": section.strip(),
                    "meta": {
                        "source": file_path.name,
                        "section_id": i
                    }
                })
        
        logger.info(f"ğŸ“„ ä» {file_path.name} åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£æ®µ")
        return docs
    except Exception as e:
        logger.error(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return []


def create_vector_db_with_progress(
    documents: List[Document],
    embeddings,
    db_path: str,
    collection_name: str,
    batch_size: int = 100
):
    """åˆ›å»ºå‘é‡åº“ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºå’Œæ‰¹å¤„ç†ï¼‰"""
    # åˆ é™¤æ—§æ•°æ®åº“
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
        logger.info(f"   â†’ å·²åˆ é™¤æ—§å‘é‡åº“")
    
    total_docs = len(documents)
    logger.info(f"   â†’ å¼€å§‹åˆ›å»ºå‘é‡åº“: {total_docs} ä¸ªæ–‡æ¡£")
    logger.info(f"   â†’ æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # ä¼°ç®—æ—¶é—´
    estimated_time = (total_docs / batch_size) * 2
    logger.info(f"   â†’ é¢„è®¡è€—æ—¶: {estimated_time/60:.1f} åˆ†é’Ÿ")
    
    db = None
    start_time = time.time()
    
    # åˆ†æ‰¹å¤„ç†
    if TQDM_AVAILABLE:
        iterator = tqdm(range(0, total_docs, batch_size), desc=f"   åˆ›å»º {collection_name}", unit="batch")
    else:
        iterator = range(0, total_docs, batch_size)
    
    for i in iterator:
        batch = documents[i:i+batch_size]
        
        if db is None:
            # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºæ•°æ®åº“
            db = Chroma.from_documents(
                documents=batch,
                embedding=embeddings,
                persist_directory=db_path,
                collection_name=collection_name,
                collection_metadata={"hnsw:space": "cosine"}
            )
        else:
            # åç»­æ‰¹æ¬¡ï¼šæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“
            db.add_documents(batch)
    
    elapsed = time.time() - start_time
    logger.info(f"   â†’ å®Œæˆï¼å®é™…è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    
    return db


# --- ä¿®å¤2ï¼šç»Ÿä¸€å‘é‡åº“åˆ›å»ºé€»è¾‘ï¼ˆæŒ‡å®šä½™å¼¦è·ç¦»ï¼‰ ---
def create_chroma_db_with_cosine(docs, db_path, collection_name):
    """
    åˆ›å»ºæŒ‡å®šä½™å¼¦è·ç¦»çš„Chromaå‘é‡åº“
    :param docs: æ–‡æ¡£åˆ—è¡¨ï¼ˆä¸èƒ½ä¸ºç©ºï¼‰
    :param db_path: æŒä¹…åŒ–è·¯å¾„
    :param collection_name: é›†åˆåç§°
    :return: Chromaå‘é‡åº“å®ä¾‹
    """
    # å¦‚æœè·¯å¾„å­˜åœ¨ï¼Œå…ˆåˆ é™¤ï¼ˆç¡®ä¿é‡æ–°åˆ›å»ºæ—¶ä½¿ç”¨æŒ‡å®šçš„è·ç¦»å‡½æ•°ï¼‰
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
        print(f"å·²åˆ é™¤æ—§å‘é‡åº“ï¼š{db_path}")

    # æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨æ˜¯å¦ä¸ºç©º
    if not docs:
        raise ValueError(f"æ— æ³•åˆ›å»ºç©ºçš„å‘é‡åº“ {db_path}ï¼Œæ–‡æ¡£åˆ—è¡¨ä¸èƒ½ä¸ºç©º")

    # åˆ›å»ºå‘é‡åº“ï¼ˆæ˜¾å¼æŒ‡å®šä½™å¼¦è·ç¦»ï¼‰
    db = Chroma.from_documents(
        documents=docs,
        embedding=EMBEDDINGS,
        persist_directory=db_path,
        collection_name=collection_name,
        collection_metadata={"hnsw:space": "cosine"}  # å¼ºåˆ¶ä½¿ç”¨ä½™å¼¦è·ç¦»
    )
    db.persist()
    print(f"âœ… å·²åˆ›å»ºä½™å¼¦è·ç¦»å‘é‡åº“ï¼š{db_path}ï¼ŒåŒ…å« {len(docs)} ä¸ªæ–‡æ¡£")
    return db


# --- 2. æ ¸å¿ƒåŠ è½½å™¨ï¼šä¿®å¤ JSON è§£æ ---
def load_single_document(file_path):
    ext = os.path.splitext(file_path)[-1].lower()

    def get_encoding(path):
        with open(path, 'rb') as f:
            return chardet.detect(f.read())['encoding'] or 'utf-8'

    try:
        if ext in [".txt", ".md"]:
            return TextLoader(file_path, encoding=get_encoding(file_path)).load()
        elif ext == ".json":
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                docs = []
                rows = []
                # å…¼å®¹æ€§ä¿®å¤ï¼šé€‚é…å•æ¡è®°å½• JSON æˆ– åˆ—è¡¨ JSON
                if isinstance(data, list):
                    rows = data
                elif isinstance(data, dict):
                    # å¦‚æœå­—å…¸æœ¬èº«å°±æœ‰è¿™äº›åŒ»å­¦é”®ï¼Œè¯´æ˜å®ƒæœ¬èº«å°±æ˜¯ä¸€è¡Œè®°å½•
                    if "medicalRecordId" in data or "ä¸»è¯‰" in data:
                        rows = [data]
                    else:
                        # å¦åˆ™æŒ‰ Sheet ç»“æ„å¤„ç†
                        for val in data.values():
                            if isinstance(val, list): rows.extend(val)
                for row in rows:
                    content = " | ".join([f"{k}: {str(v).strip()}" for k, v in row.items() if v])
                    content = re.sub(r'\s+', ' ', content)
                    if len(content) > 10:
                        docs.append(Document(
                            page_content=content,
                            metadata={"source": file_path}
                        ))
                return docs
        return []
    except Exception as e:
        print(f"\nâŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        return []


# --- 3. å¢é‡åŒæ­¥é€»è¾‘ï¼šé€‚é…æ ¹ç›®å½•chroma + ä½™å¼¦è·ç¦» ---
def update_vector_db(db_name, data_folder, use_dynamic_chunker=True):
    """
    å¢é‡æ›´æ–°å‘é‡åº“
    :param db_name: æ•°æ®åº“åç§°
    :param data_folder: æ•°æ®æ–‡ä»¶å¤¹
    :param use_dynamic_chunker: æ˜¯å¦ä½¿ç”¨åŠ¨æ€åˆ†å—å™¨ï¼ˆé»˜è®¤Trueï¼‰
    """
    # å‘é‡åº“è·¯å¾„ï¼šæ ¹ç›®å½•/chroma/{db_name}
    db_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chroma", db_name)
    data_dir = os.path.join("data", data_folder)
    print(f"\n>>> ğŸš€ å¼€å§‹åŒæ­¥æ•°æ®åº“: {db_name}")
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
        return

    # è·å–æ–‡ä»¶åˆ—è¡¨
    allowed_extensions = {".txt", ".md", ".json"}
    files_to_process = [f for f in os.listdir(data_dir) if os.path.splitext(f)[-1].lower() in allowed_extensions]

    if not files_to_process:
        print(f"âš ï¸ æ•°æ®ç›®å½• {data_dir} ä¸­æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶")
        return

    # æ ¹æ®å‚æ•°é€‰æ‹©åˆ†å—å™¨
    if use_dynamic_chunker and DYNAMIC_CHUNKER_AVAILABLE:
        print(f"ğŸ“Š ä½¿ç”¨åŠ¨æ€åˆ†å—ç­–ç•¥")
        chunker = DynamicChunker()
        # æ ¹æ®æ•°æ®åº“ç±»å‹é€‰æ‹©åˆ†å—é…ç½®
        if "Guide" in db_name:
            chunk_config = ChunkConfig(
                strategy=ChunkStrategy.HIERARCHICAL,
                chunk_size=800,
                chunk_overlap=100,
                min_chunk_size=100,
                max_chunk_size=2000
            )
        elif "Case" in db_name:
            chunk_config = ChunkConfig(
                strategy=ChunkStrategy.SEMANTIC,
                chunk_size=600,
                chunk_overlap=80,
                min_chunk_size=100,
                max_chunk_size=1500
            )
        else:
            chunk_config = ChunkConfig(
                strategy=ChunkStrategy.FIXED,
                chunk_size=600,
                chunk_overlap=60
            )
        use_dynamic = True
    else:
        if use_dynamic_chunker and not DYNAMIC_CHUNKER_AVAILABLE:
            print(f"âš ï¸ åŠ¨æ€åˆ†å—å™¨ä¸å¯ç”¨ï¼Œä½¿ç”¨å›ºå®šåˆ†å—ç­–ç•¥")
        else:
            print(f"ğŸ“Š ä½¿ç”¨å›ºå®šåˆ†å—ç­–ç•¥")
        # å›ºå®šåˆ‡åˆ†å™¨ï¼šå¼ºåˆ¶ç”¨äºæ‰€æœ‰æ–‡ä»¶ï¼Œé˜²æ­¢ Token æº¢å‡º
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=600,
            chunk_overlap=60,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""]
        )
        use_dynamic = False

    # æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨
    if os.path.exists(db_path):
        # åŠ è½½å·²æœ‰å‘é‡åº“
        db = Chroma(
            persist_directory=db_path,
            embedding_function=EMBEDDINGS,
            collection_name=db_name,
            collection_metadata={"hnsw:space": "cosine"}
        )

        # è·å–å·²å­˜åœ¨æ–‡ä»¶æ¸…å•
        results = db.get()
        processed_files = set()
        if results and results['metadatas']:
            processed_files = {os.path.basename(m['source']) for m in results['metadatas'] if 'source' in m}
    else:
        # é¦–æ¬¡åˆ›å»ºå‘é‡åº“
        db = None
        processed_files = set()
        print(f"ğŸ†• é¦–æ¬¡åˆ›å»ºå‘é‡åº“: {db_name}")

    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    for i, filename in enumerate(files_to_process):
        # å¦‚æœæ–‡ä»¶å·²åœ¨åº“ä¸­ï¼Œç›´æ¥è·³è¿‡
        if filename in processed_files:
            print(f"â© æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            continue

        file_path = os.path.join(data_dir, filename)
        print(f"ğŸ“„ æ­£åœ¨å¤„ç† ({i + 1}/{len(files_to_process)}): {filename} ", end="", flush=True)

        raw_docs = load_single_document(file_path)
        if raw_docs:
            # æ ¹æ®åˆ†å—ç­–ç•¥è¿›è¡Œåˆ‡åˆ†
            if use_dynamic:
                # è½¬æ¢ä¸ºåŠ¨æ€åˆ†å—å™¨éœ€è¦çš„æ ¼å¼
                docs_for_chunking = []
                for doc in raw_docs:
                    docs_for_chunking.append({
                        "text": doc.page_content,
                        "meta": doc.metadata
                    })
                chunked = chunker.chunk_documents(docs_for_chunking, chunk_config)
                # è½¬æ¢å› LangChain Document
                current_splits = [
                    Document(page_content=d["text"], metadata=d["meta"])
                    for d in chunked
                ]
            else:
                # ä½¿ç”¨å›ºå®šåˆ†å—
                current_splits = text_splitter.split_documents(raw_docs)
            
            if current_splits:
                if db is None:
                    # é¦–æ¬¡åˆ›å»ºå‘é‡åº“
                    db = create_chroma_db_with_cosine(current_splits, db_path, db_name)
                else:
                    # æ‰¹é‡å…¥åº“åˆ°å·²æœ‰å‘é‡åº“
                    batch_size = 50
                    for j in range(0, len(current_splits), batch_size):
                        batch = current_splits[j: j + batch_size]
                        db.add_documents(batch)
                        print(".", end="", flush=True)
                    db.persist()
                print(f" âœ… å®Œæˆ (æ–°å¢ {len(current_splits)} ä¸ªç‰‡æ®µ)")
            else:
                print("âš ï¸ å†…å®¹æ— æ•ˆ")
        else:
            print("âŒ åŠ è½½å¤±è´¥")

    print(f"âœ¨ {db_name} åŒæ­¥å®Œæˆï¼")


# --- 4. å®æ—¶å¯¹è¯å­˜å‚¨å‡½æ•°ï¼šä¿®å¤ç‰ˆï¼ˆç¡®ä¿ä½™å¼¦è·ç¦»ï¼‰ ---
def store_chat_history_rag(question: str, answer: str, patient_id: str, db_name="UserHistory_db"):
    """
    å¢åŠ äº† patient_id å‚æ•°ï¼Œå®ç°å¤šæ‚£è€…éš”ç¦»å­˜å‚¨ï¼Œå‘é‡åº“æŒ‡å‘æ ¹ç›®å½•chroma
    ä¿®å¤ï¼šå¼ºåˆ¶ä½¿ç”¨ä½™å¼¦è·ç¦»å­˜å‚¨
    """
    # æ„å»ºæ–‡æ¡£
    doc_content = f"æ‚£è€…é—®: {question} | åŒ»ç”Ÿç­”: {answer}"
    doc = Document(
        page_content=doc_content,
        metadata={"patient_id": patient_id, "source": "conversation_history"}
    )

    # å‘é‡åº“è·¯å¾„ï¼šæ ¹ç›®å½•/chroma/{db_name}
    root_dir = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(root_dir, "chroma", db_name)

    # åˆ‡ç‰‡
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=600,
        chunk_overlap=60,
        separators=["|", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " "]
    )
    splits = text_splitter.split_documents([doc])

    if not splits:
        print(f"âš ï¸ æ— æ³•å­˜å‚¨ç©ºå¯¹è¯")
        return

    # æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨
    if os.path.exists(db_path):
        # åŠ è½½å·²æœ‰å‘é‡åº“
        db = Chroma(
            persist_directory=db_path,
            embedding_function=EMBEDDINGS,
            collection_name="UserHistory",
            collection_metadata={"hnsw:space": "cosine"}
        )
        db.add_documents(splits)
        db.persist()
    else:
        # é¦–æ¬¡åˆ›å»ºå‘é‡åº“
        create_chroma_db_with_cosine(
            docs=splits,
            db_path=db_path,
            collection_name="UserHistory"
        )

    print(f"âœ… æ‚£è€… {patient_id} çš„å¯¹è¯å·²å®Œæˆåˆ‡ç‰‡ç´¢å¼•")


# --- 5. åŒ»ç”Ÿè¿›åŒ–å­˜å‚¨å‡½æ•°ï¼šåŒå­˜å‚¨ï¼ˆç”¨æˆ·ä¸“å±+å…¨é‡æ±‡æ€»ï¼‰ï¼Œé€‚é…state/dataset ---
def store_doctor_qa_evolution(question, answer, rag_info, patient_id, score, is_high_quality):
    """
    ä»…é«˜è´¨é‡å¯¹è¯æ‰ä¼šè¢«å­˜å…¥CSVï¼Œç”¨äºFew-shotå­¦ä¹ 
    æ–°å¢ï¼šåŒæ­¥å†™å…¥ã€state/datasetã€‘ä¸‹çš„ç”¨æˆ·ä¸“å±CSV + å…¨é‡é«˜åˆ†å¯¹è¯æ±‡æ€»CSVï¼Œæ ‡è®°patient_idä¾¿äºæº¯æº
    """
    if not is_high_quality:
        print(f"âš ï¸ å¯¹è¯éé«˜è´¨é‡ï¼ˆæ€»åˆ†{score}ï¼‰ï¼Œæœªè¾¾è¿›åŒ–æ ‡å‡†ï¼Œä¸æ‰§è¡Œ CSV å­˜å‚¨")
        return

    # CSVå­˜å‚¨è·¯å¾„ï¼šæ ¹ç›®å½•/state/dataset/
    root_dir = os.path.dirname(os.path.abspath(__file__))
    csv_dir = os.path.join(root_dir, "state", "dataset")
    os.makedirs(csv_dir, exist_ok=True)

    # ========== 1. å†™å…¥ç”¨æˆ·ä¸“å±CSV ==========
    user_csv_path = os.path.join(csv_dir, f"doctor_evolve_{patient_id}.csv")
    file_exists = os.path.isfile(user_csv_path)
    # æ–°å¢patient_idå­—æ®µï¼Œç”¨äºæº¯æºå’Œå‘é‡åº“å»é‡
    headers = [
        "patient_id",
        "question1", "qus_embedding", "rag_info1", "answer1",
        "qus2_embedding", "question2", "answer2", "rag_info2",
        "total_score", "is_high_quality"
    ]
    # æ„é€ æ•°æ®è¡Œ
    row = [
        patient_id,
        "N/A", "vector_placeholder", "N/A", "N/A",
        "vector_placeholder", question, answer, rag_info,
        score, is_high_quality
    ]
    # å†™å…¥ç”¨æˆ·ä¸“å±åº“
    with open(user_csv_path, 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(headers)
        writer.writerow(row)

    # ========== 2. æ–°å¢ï¼šå†™å…¥å…¨é‡é«˜åˆ†å¯¹è¯æ±‡æ€»CSV ==========
    summary_csv_path = os.path.join(csv_dir, "doctor_evolve_summary.csv")
    summary_file_exists = os.path.isfile(summary_csv_path)
    with open(summary_csv_path, 'a', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        if not summary_file_exists:
            writer.writerow(headers)  # å­—æ®µä¸ç”¨æˆ·CSVä¸€è‡´ï¼Œå«patient_id
        writer.writerow(row)  # å¤ç”¨åŒä¸€è¡Œæ•°æ®ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´
    print(f"ğŸš€ é«˜è´¨é‡é—®ç­”å·²å­˜å…¥ï¼šç”¨æˆ·ä¸“å±åº“({user_csv_path}) + å…¨é‡æ±‡æ€»åº“({summary_csv_path}) (Score: {score})")


# --- 6. ä¿®å¤3ï¼šé«˜è´¨é‡é—®ç­”å‘é‡åº“åˆå§‹åŒ–/æ›´æ–°å‡½æ•°ï¼ˆä½™å¼¦è·ç¦»ç‰ˆï¼‰ ---
def init_high_quality_qa_db():
    """
    åˆå§‹åŒ–/æ›´æ–°é«˜è´¨é‡é—®ç­”å‘é‡åº“ï¼ˆHighQualityQA_dbï¼‰
    ã€ä¿®å¤ç‰ˆã€‘ï¼šä½¿ç”¨ä½™å¼¦è·ç¦»+å½’ä¸€åŒ–åµŒå…¥ï¼Œå°†é—®é¢˜å’Œç­”æ¡ˆåˆ†å¼€å­˜å‚¨
    åŸºäºã€state/dataset/doctor_evolve_summary.csvã€‘æ„å»ºï¼Œå‘é‡åº“æŒ‡å‘æ ¹ç›®å½•chroma
    ã€å¢å¼ºã€‘ï¼šå³ä½¿ CSV ä¸å­˜åœ¨ä¹Ÿä¼šåˆ›å»ºç©ºå‘é‡åº“
    """
    db_name = "HighQualityQA_db"
    # å‘é‡åº“è·¯å¾„ï¼šæ ¹ç›®å½•/chroma/HighQualityQA_db
    root_dir = os.path.dirname(os.path.abspath(__file__))
    db_path = os.path.join(root_dir, "chroma", db_name)
    # CSVæ±‡æ€»æ–‡ä»¶è·¯å¾„ï¼šæ ¹ç›®å½•/state/dataset/doctor_evolve_summary.csv
    csv_dir = os.path.join(root_dir, "state", "dataset")
    summary_csv_path = os.path.join(csv_dir, "doctor_evolve_summary.csv")

    # 1. æ£€æŸ¥æ±‡æ€»CSVæ˜¯å¦å­˜åœ¨
    if not os.path.exists(summary_csv_path):
        logger.warning(f"âš ï¸ å…¨é‡é«˜åˆ†å¯¹è¯æ±‡æ€»CSVä¸å­˜åœ¨({summary_csv_path})ï¼Œåˆ›å»ºç©ºå‘é‡åº“")
        # åˆ›å»ºç©ºå‘é‡åº“
        if not os.path.exists(db_path):
            placeholder_doc = Document(
                page_content="é«˜è´¨é‡é—®ç­”åº“åˆå§‹åŒ–å ä½ç¬¦",
                metadata={
                    "type": "placeholder",
                    "patient_id": "system",
                    "question": "åˆå§‹åŒ–é—®é¢˜",
                    "answer": "åˆå§‹åŒ–ç­”æ¡ˆ",
                    "source": "high_quality_qa_init"
                }
            )
            db = Chroma.from_documents(
                documents=[placeholder_doc],
                embedding=EMBEDDINGS,
                persist_directory=db_path,
                collection_name="HighQualityQA",
                collection_metadata={"hnsw:space": "cosine"}
            )
            logger.info(f"âœ… é«˜è´¨é‡é—®ç­”åº“åˆ›å»ºæˆåŠŸï¼ˆç©ºåº“ï¼‰")
        else:
            logger.info(f"â„¹ï¸  é«˜è´¨é‡é—®ç­”åº“å·²å­˜åœ¨ï¼Œæ— éœ€æ›´æ–°")
        return

    # 2. è¯»å–CSVå¹¶æ„é€ æ–‡æ¡£
    high_quality_docs = []
    with open(summary_csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            question = row["question2"]
            answer = row["answer2"]
            patient_id = row["patient_id"]

            # âš¡ï¸ å…³é”®æ”¹è¿›ï¼šå°†é—®é¢˜å•ç‹¬å­˜å‚¨ä¸ºä¸€ä¸ªæ–‡æ¡£ï¼ˆä¾¿äºé—®é¢˜åŒ¹é…ï¼‰
            question_doc = Document(
                page_content=question,  # åªå­˜å‚¨é—®é¢˜æ–‡æœ¬ï¼Œä¾¿äºåŒ¹é…
                metadata={
                    "patient_id": patient_id,
                    "question": question,
                    "answer": answer,  # ç­”æ¡ˆæ”¾åœ¨metadataä¸­
                    "full_answer": answer,
                    "score": row["total_score"],
                    "source": "high_quality_qa_summary",
                    "doc_type": "question"  # æ ‡è®°æ–‡æ¡£ç±»å‹
                }
            )
            high_quality_docs.append(question_doc)

            # å¯é€‰ï¼šä¹Ÿå¯ä»¥å­˜å‚¨ç­”æ¡ˆæ–‡æ¡£ï¼Œç”¨äºç­”æ¡ˆæ£€ç´¢
            answer_doc = Document(
                page_content=answer,  # å­˜å‚¨ç­”æ¡ˆæ–‡æœ¬
                metadata={
                    "patient_id": patient_id,
                    "question": question,
                    "answer": answer,
                    "score": row["total_score"],
                    "source": "high_quality_qa_summary",
                    "doc_type": "answer"
                }
            )
            high_quality_docs.append(answer_doc)

    if not high_quality_docs:
        logger.warning("â„¹ï¸ æ— é«˜è´¨é‡é—®ç­”æ•°æ®ï¼Œåˆ›å»ºç©ºå‘é‡åº“")
        # åˆ›å»ºç©ºå‘é‡åº“
        if not os.path.exists(db_path):
            placeholder_doc = Document(
                page_content="é«˜è´¨é‡é—®ç­”åº“åˆå§‹åŒ–å ä½ç¬¦",
                metadata={
                    "type": "placeholder",
                    "patient_id": "system",
                    "question": "åˆå§‹åŒ–é—®é¢˜",
                    "answer": "åˆå§‹åŒ–ç­”æ¡ˆ",
                    "source": "high_quality_qa_init"
                }
            )
            db = Chroma.from_documents(
                documents=[placeholder_doc],
                embedding=EMBEDDINGS,
                persist_directory=db_path,
                collection_name="HighQualityQA",
                collection_metadata={"hnsw:space": "cosine"}
            )
            logger.info(f"âœ… é«˜è´¨é‡é—®ç­”åº“åˆ›å»ºæˆåŠŸï¼ˆç©ºåº“ï¼‰")
        return

    # 3. æ‰¹é‡å…¥åº“ï¼ˆåˆ‡ç‰‡åå…¥åº“ï¼Œé˜²æ­¢Tokenæº¢å‡ºï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=50,
        separators=["|", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " "]
    )
    splits = text_splitter.split_documents(high_quality_docs)

    if not splits:
        print("âš ï¸ åˆ‡ç‰‡åæ— æœ‰æ•ˆæ–‡æ¡£")
        return

    # 4. åˆå§‹åŒ–/åŠ è½½å‘é‡åº“ï¼ˆæŒ‡å®šä½™å¼¦è·ç¦»ï¼‰
    if os.path.exists(db_path):
        # åŠ è½½å·²æœ‰å‘é‡åº“
        db = Chroma(
            persist_directory=db_path,
            embedding_function=EMBEDDINGS,
            collection_name="HighQualityQA",
            collection_metadata={"hnsw:space": "cosine"}
        )

        # è¯»å–å·²å­˜åœ¨çš„è®°å½•ï¼ˆé€šè¿‡question+patient_idåšå”¯ä¸€æ ‡è¯†ï¼‰
        processed_qa = set()
        results = db.get()
        if results and results['metadatas']:
            processed_qa = {(m.get("question"), m.get("patient_id")) for m in results['metadatas'] if
                            m.get("question") and m.get("patient_id")}

        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ–‡æ¡£
        new_splits = []
        for split in splits:
            question = split.metadata.get("question")
            patient_id = split.metadata.get("patient_id")
            if (question, patient_id) not in processed_qa:
                new_splits.append(split)

        splits = new_splits
    else:
        # é¦–æ¬¡åˆ›å»ºå‘é‡åº“
        db = create_chroma_db_with_cosine(splits, db_path, "HighQualityQA")
        return

    # 5. æ‰¹é‡å…¥åº“
    if splits:
        batch_size = 50
        for j in range(0, len(splits), batch_size):
            batch = splits[j:j + batch_size]
            db.add_documents(batch)
        db.persist()
        print(f"âœ… é«˜è´¨é‡é—®ç­”å‘é‡åº“æ›´æ–°å®Œæˆï¼Œæ–°å¢ {len(splits)} ä¸ªç‰‡æ®µï¼ˆé—®é¢˜+ç­”æ¡ˆåˆ†åˆ«å­˜å‚¨ï¼‰")
    else:
        print("â„¹ï¸ æ— æ–°å¢é«˜è´¨é‡é—®ç­”ï¼Œå‘é‡åº“æ— éœ€æ›´æ–°")


# =============================================================================
# Rebuild æ¨¡å¼ï¼šä½¿ç”¨åŠ¨æ€åˆ†å—å®Œå…¨é‡å»ºå‘é‡åº“
# =============================================================================

def rebuild_medical_guide_db_dynamic():
    """é‡å»ºåŒ»å­¦æŒ‡å—åº“ï¼ˆä½¿ç”¨åŠ¨æ€åˆ†å— - å±‚æ¬¡åˆ†å—ï¼‰"""
    if not DYNAMIC_CHUNKER_AVAILABLE:
        logger.warning("âš ï¸  DynamicChunker ä¸å¯ç”¨ï¼Œä½¿ç”¨å›ºå®šåˆ†å—")
        update_vector_db("MedicalGuide_db", "MedicalGuide_data")
        return
    
    logger.info("ğŸ—ï¸  å¼€å§‹é‡å»ºï¼šåŒ»å­¦æŒ‡å—åº“ï¼ˆåŠ¨æ€åˆ†å—ï¼‰")
    
    data_dir = Path(CURRENT_DIR) / "data" / "MedicalGuide_data"
    output_dir = Path(CURRENT_DIR) / "chroma"
    
    if not data_dir.exists():
        logger.warning(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åŠ è½½æ•°æ®
    guide_files = list(data_dir.glob("*.txt"))
    all_docs = []
    
    for file in guide_files:
        docs = load_documents_from_txt_rebuild(file)
        for doc in docs:
            doc["meta"]["type"] = "guideline"
        all_docs.extend(docs)
    
    if not all_docs:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ»å­¦æŒ‡å—æ–‡æ¡£")
        return
    
    # ä½¿ç”¨å±‚æ¬¡åˆ†å—ç­–ç•¥
    chunker = DynamicChunker()
    config = ChunkConfig(
        strategy=ChunkStrategy.HIERARCHICAL,
        chunk_size=800,
        chunk_overlap=100,
        min_chunk_size=100,
        max_chunk_size=2000
    )
    
    chunked_docs = chunker.chunk_documents(all_docs, config)
    
    # è½¬æ¢ä¸º LangChain Document æ ¼å¼
    lc_docs = [
        Document(page_content=doc["text"], metadata=doc["meta"])
        for doc in chunked_docs
    ]
    
    # åˆ›å»ºå‘é‡åº“
    db_path = str(output_dir / "MedicalGuide_db")
    create_vector_db_with_progress(
        documents=lc_docs,
        embeddings=EMBEDDINGS,
        db_path=db_path,
        collection_name="MedicalGuide",
        batch_size=100
    )
    
    logger.info(f"âœ… åŒ»å­¦æŒ‡å—åº“åˆ›å»ºæˆåŠŸ: {len(lc_docs)} ä¸ªå—")


def rebuild_hospital_process_db_dynamic():
    """é‡å»ºåŒ»é™¢æµç¨‹åº“ï¼ˆä½¿ç”¨åŠ¨æ€åˆ†å— - å›ºå®šåˆ†å—é€‚ç”¨äºæ¨¡æ¿ç±»æ–‡æ¡£ï¼‰"""
    if not DYNAMIC_CHUNKER_AVAILABLE:
        logger.warning("âš ï¸  DynamicChunker ä¸å¯ç”¨ï¼Œä½¿ç”¨å›ºå®šåˆ†å—")
        update_vector_db("HospitalProcess_db", "HospitalProcess_data")
        return
    
    logger.info("ğŸ—ï¸  å¼€å§‹é‡å»ºï¼šåŒ»é™¢æµç¨‹åº“ï¼ˆåŠ¨æ€åˆ†å—ï¼‰")
    
    data_dir = Path(CURRENT_DIR) / "data" / "HospitalProcess_data"
    output_dir = Path(CURRENT_DIR) / "chroma"
    
    if not data_dir.exists():
        logger.warning(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åŠ è½½æ•°æ®ï¼ˆæ”¯æŒ txt å’Œ json æ ¼å¼ï¼‰
    process_files = list(data_dir.glob("*.txt")) + list(data_dir.glob("*.md")) + list(data_dir.glob("*.json"))
    all_docs = []
    
    for file in process_files:
        if file.suffix in [".txt", ".md"]:
            docs = load_documents_from_txt_rebuild(file)
            for doc in docs:
                doc["meta"]["type"] = "hospital_process"
            all_docs.extend(docs)
        elif file.suffix == ".json":
            docs = load_documents_from_json_rebuild(file)
            for item in docs:
                doc = None
                if isinstance(item, dict):
                    if "text" in item or "content" in item:
                        doc = {
                            "text": item.get("text") or item.get("content", ""),
                            "meta": item.get("meta", {})
                        }
                    else:
                        doc = {"text": str(item), "meta": {}}
                elif isinstance(item, str):
                    doc = {"text": item, "meta": {}}
                
                if doc and doc["text"].strip():
                    doc["meta"]["type"] = "hospital_process"
                    doc["meta"]["source"] = file.name
                    all_docs.append(doc)
    
    if not all_docs:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°åŒ»é™¢æµç¨‹æ–‡æ¡£")
        return
    
    # ä½¿ç”¨å›ºå®šåˆ†å—ç­–ç•¥ï¼ˆé€‚åˆæ¨¡æ¿å’Œæµç¨‹æ–‡æ¡£ï¼‰
    chunker = DynamicChunker()
    config = ChunkConfig(
        strategy=ChunkStrategy.FIXED,
        chunk_size=600,
        chunk_overlap=60,
        min_chunk_size=100,
        max_chunk_size=1500
    )
    
    chunked_docs = chunker.chunk_documents(all_docs, config)
    
    # è½¬æ¢ä¸º LangChain Document æ ¼å¼
    lc_docs = [
        Document(page_content=doc["text"], metadata=doc["meta"])
        for doc in chunked_docs
    ]
    
    # åˆ›å»ºå‘é‡åº“
    db_path = str(output_dir / "HospitalProcess_db")
    create_vector_db_with_progress(
        documents=lc_docs,
        embeddings=EMBEDDINGS,
        db_path=db_path,
        collection_name="HospitalProcess",
        batch_size=100
    )
    
    logger.info(f"âœ… åŒ»é™¢æµç¨‹åº“åˆ›å»ºæˆåŠŸ: {len(lc_docs)} ä¸ªå—")


def rebuild_clinical_case_db_dynamic():
    """é‡å»ºä¸´åºŠæ¡ˆä¾‹åº“ï¼ˆä½¿ç”¨åŠ¨æ€åˆ†å— - è¯­ä¹‰åˆ†å—ï¼‰"""
    if not DYNAMIC_CHUNKER_AVAILABLE:
        logger.warning("âš ï¸  DynamicChunker ä¸å¯ç”¨ï¼Œä½¿ç”¨å›ºå®šåˆ†å—")
        update_vector_db("ClinicalCase_db", "ClinicalCase_data")
        return
    
    logger.info("ğŸ—ï¸  å¼€å§‹é‡å»ºï¼šä¸´åºŠæ¡ˆä¾‹åº“ï¼ˆåŠ¨æ€åˆ†å—ï¼‰")
    
    data_dir = Path(CURRENT_DIR) / "data" / "ClinicalCase_data"
    output_dir = Path(CURRENT_DIR) / "chroma"
    
    if not data_dir.exists():
        logger.warning(f"âš ï¸  ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # åŠ è½½æ•°æ®
    case_files = list(data_dir.glob("*.json"))
    all_docs = []
    
    for file in case_files:
        raw_docs = load_documents_from_json_rebuild(file)
        
        for item in raw_docs:
            doc = None
            
            if isinstance(item, dict):
                if "text" in item or "content" in item:
                    doc = {
                        "text": item.get("text") or item.get("content", ""),
                        "meta": item.get("meta", {})
                    }
                else:
                    doc = {"text": str(item), "meta": {}}
            elif isinstance(item, str):
                doc = {"text": item, "meta": {}}
            else:
                logger.warning(f"   âš ï¸  è·³è¿‡ä¸æ”¯æŒçš„æ•°æ®ç±»å‹: {type(item)}")
                continue
            
            if doc and doc["text"].strip():
                doc["meta"]["type"] = "case"
                doc["meta"]["source"] = file.name
                all_docs.append(doc)
    
    if not all_docs:
        logger.warning("âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä¸´åºŠæ¡ˆä¾‹æ–‡æ¡£")
        return
    
    # ä½¿ç”¨è¯­ä¹‰åˆ†å—ç­–ç•¥
    chunker = DynamicChunker()
    config = ChunkConfig(
        strategy=ChunkStrategy.SEMANTIC,
        chunk_size=600,
        chunk_overlap=80,
        min_chunk_size=100,
        max_chunk_size=1500
    )
    
    chunked_docs = chunker.chunk_documents(all_docs, config)
    
    # è½¬æ¢ä¸º LangChain Document æ ¼å¼
    lc_docs = [
        Document(page_content=doc["text"], metadata=doc["meta"])
        for doc in chunked_docs
    ]
    
    # åˆ›å»ºå‘é‡åº“
    db_path = str(output_dir / "ClinicalCase_db")
    create_vector_db_with_progress(
        documents=lc_docs,
        embeddings=EMBEDDINGS,
        db_path=db_path,
        collection_name="ClinicalCase",
        batch_size=100
    )
    
    logger.info(f"âœ… ä¸´åºŠæ¡ˆä¾‹åº“åˆ›å»ºæˆåŠŸ: {len(lc_docs)} ä¸ªå—")


def rebuild_all_databases():
    """ä½¿ç”¨åŠ¨æ€åˆ†å—é‡å»ºæ‰€æœ‰å‘é‡åº“"""
    print("=" * 70)
    logger.info("=" * 60)
    logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨åŠ¨æ€åˆ†å—ç­–ç•¥é‡å»ºå‘é‡åº“")
    logger.info("=" * 60)
    print()
    
    total_start = time.time()
    
    # é‡å»ºå„ä¸ªåº“
    rebuild_medical_guide_db_dynamic()
    print()
    rebuild_hospital_process_db_dynamic()
    print()
    rebuild_clinical_case_db_dynamic()
    print()
    
    # é«˜è´¨é‡é—®ç­”åº“ä½¿ç”¨åŸæœ‰é€»è¾‘
    logger.info("ğŸ—ï¸  å¼€å§‹é‡å»ºï¼šé«˜è´¨é‡é—®ç­”åº“")
    init_high_quality_qa_db()
    print()
    
    # ç”¨æˆ·å†å²åº“
    logger.info("ğŸ—ï¸  å¼€å§‹é‡å»ºï¼šç”¨æˆ·å†å²åº“")
    db_path = os.path.join(CURRENT_DIR, "chroma", "UserHistory_db")
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
    placeholder_doc = Document(
        page_content="æ‚£è€…å†å²è®°å¿†åº“åˆå§‹åŒ–",
        metadata={"type": "placeholder", "patient_id": "system"}
    )
    db = Chroma.from_documents(
        documents=[placeholder_doc],
        embedding=EMBEDDINGS,
        persist_directory=db_path,
        collection_name="UserHistory",
        collection_metadata={"hnsw:space": "cosine"}
    )
    logger.info(f"âœ… ç”¨æˆ·å†å²åº“åˆ›å»ºæˆåŠŸï¼ˆç©ºåº“ï¼‰")
    print()
    
    total_elapsed = time.time() - total_start
    
    print("=" * 70)
    logger.info("=" * 60)
    logger.info("âœ… æ‰€æœ‰å‘é‡åº“é‡å»ºå®Œæˆï¼")
    logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_elapsed/60:.1f} åˆ†é’Ÿ")
    logger.info("=" * 60)
    print("=" * 70)


# =============================================================================
# ä¸»å‡½æ•°ï¼šæ”¯æŒå‘½ä»¤è¡Œå‚æ•°
# =============================================================================

def main():
    """ä¸»å‡½æ•°ï¼šæ”¯æŒ rebuild å’Œupdate ä¸¤ç§æ¨¡å¼ï¼Œæ”¯æŒåŠ¨æ€/å›ºå®šåˆ†å—"""
    parser = argparse.ArgumentParser(
        description="å‘é‡åº“ç®¡ç†å·¥å…·ï¼šæ”¯æŒå¢é‡æ›´æ–°å’Œå®Œå…¨é‡å»ºï¼Œæ”¯æŒåŠ¨æ€/å›ºå®šåˆ†å—",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆé»˜è®¤ï¼Œä½¿ç”¨åŠ¨æ€åˆ†å—ï¼‰
  python create_database_general.py
  python create_database_general.py --mode update --chunker dynamic
  
  # å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆä½¿ç”¨å›ºå®šåˆ†å—ï¼‰
  python create_database_general.py --chunker fixed
  
  # å®Œå…¨é‡å»ºæ¨¡å¼ï¼ˆä½¿ç”¨åŠ¨æ€åˆ†å—ï¼‰
  python create_database_general.py --mode rebuild
  
  # å®Œå…¨é‡å»ºæ¨¡å¼ï¼ˆä½¿ç”¨å›ºå®šåˆ†å—ï¼‰
  python create_database_general.py --mode rebuild --chunker fixed
  
  # åªæ›´æ–°/é‡å»ºç‰¹å®šæ•°æ®åº“
  python create_database_general.py --mode update --db guide
  python create_database_general.py --mode rebuild --db case
        """
    )
    
    parser.add_argument(
        '--mode',
        choices=['update', 'rebuild'],
        default='update',
        help='è¿è¡Œæ¨¡å¼ï¼šupdate=å¢é‡æ›´æ–°ï¼ˆé»˜è®¤ï¼‰ï¼Œrebuild=å®Œå…¨é‡å»º'
    )
    
    parser.add_argument(
        '--chunker',
        choices=['dynamic', 'fixed'],
        default='dynamic',
        help='åˆ†å—ç­–ç•¥ï¼šdynamic=åŠ¨æ€è‡ªé€‚åº”åˆ†å—ï¼ˆé»˜è®¤ï¼‰ï¼Œfixed=å›ºå®šå¤§å°åˆ†å—'
    )
    
    parser.add_argument(
        '--db',
        choices=['all', 'guide', 'process', 'case', 'qa', 'history'],
        default='all',
        help='æŒ‡å®šæ•°æ®åº“ï¼šall=å…¨éƒ¨ï¼ˆé»˜è®¤ï¼‰ï¼Œguide=åŒ»å­¦æŒ‡å—ï¼Œprocess=åŒ»é™¢æµç¨‹ï¼Œcase=ä¸´åºŠæ¡ˆä¾‹ï¼Œqa=é—®ç­”åº“ï¼Œhistory=å†å²åº“'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨åŠ¨æ€åˆ†å—
    use_dynamic = (args.chunker == 'dynamic')
    chunker_name = "åŠ¨æ€è‡ªé€‚åº”åˆ†å—" if use_dynamic else "å›ºå®šå¤§å°åˆ†å—"
    
    if args.mode == 'rebuild':
        print(f"\nğŸ“¦ è¿è¡Œæ¨¡å¼: REBUILDï¼ˆå®Œå…¨é‡å»º + {chunker_name}ï¼‰\n")
        
        if use_dynamic:
            # ä½¿ç”¨åŠ¨æ€åˆ†å—çš„é‡å»ºæ¨¡å¼
            if args.db == 'all':
                rebuild_all_databases()
            elif args.db == 'guide':
                rebuild_medical_guide_db_dynamic()
            elif args.db == 'process':
                rebuild_hospital_process_db_dynamic()
            elif args.db == 'case':
                rebuild_clinical_case_db_dynamic()
            elif args.db == 'qa':
                init_high_quality_qa_db()
            elif args.db == 'history':
                db_path = os.path.join(CURRENT_DIR, "chroma", "UserHistory_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                placeholder_doc = Document(
                    page_content="æ‚£è€…å†å²è®°å¿†åº“åˆå§‹åŒ–",
                    metadata={"type": "placeholder", "patient_id": "system"}
                )
                db = Chroma.from_documents(
                    documents=[placeholder_doc],
                    embedding=EMBEDDINGS,
                    persist_directory=db_path,
                    collection_name="UserHistory",
                    collection_metadata={"hnsw:space": "cosine"}
                )
                logger.info(f"âœ… ç”¨æˆ·å†å²åº“åˆ›å»ºæˆåŠŸï¼ˆç©ºåº“ï¼‰")
        else:
            # ä½¿ç”¨å›ºå®šåˆ†å—çš„é‡å»ºæ¨¡å¼ï¼ˆå…ˆåˆ é™¤å†å¢é‡æ›´æ–°ï¼‰
            print("ğŸ”„ ä½¿ç”¨å›ºå®šåˆ†å—è¿›è¡Œé‡å»º...")
            if args.db in ['all', 'guide']:
                db_path = os.path.join(CURRENT_DIR, "chroma", "MedicalGuide_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                    print(f"âœ… å·²åˆ é™¤æ—§å‘é‡åº“: MedicalGuide_db")
                update_vector_db("MedicalGuide_db", "MedicalGuide_data", use_dynamic_chunker=False)
            
            if args.db in ['all', 'process']:
                db_path = os.path.join(CURRENT_DIR, "chroma", "HospitalProcess_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                    print(f"âœ… å·²åˆ é™¤æ—§å‘é‡åº“: HospitalProcess_db")
                update_vector_db("HospitalProcess_db", "HospitalProcess_data", use_dynamic_chunker=False)
            
            if args.db in ['all', 'case']:
                db_path = os.path.join(CURRENT_DIR, "chroma", "ClinicalCase_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                    print(f"âœ… å·²åˆ é™¤æ—§å‘é‡åº“: ClinicalCase_db")
                update_vector_db("ClinicalCase_db", "ClinicalCase_data", use_dynamic_chunker=False)
            
            if args.db in ['all', 'qa']:
                db_path = os.path.join(CURRENT_DIR, "chroma", "HighQualityQA_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                    print(f"âœ… å·²åˆ é™¤æ—§å‘é‡åº“: HighQualityQA_db")
                init_high_quality_qa_db()
            
            if args.db in ['all', 'history']:
                db_path = os.path.join(CURRENT_DIR, "chroma", "UserHistory_db")
                if os.path.exists(db_path):
                    shutil.rmtree(db_path)
                placeholder_doc = Document(
                    page_content="æ‚£è€…å†å²è®°å¿†åº“åˆå§‹åŒ–",
                    metadata={"type": "placeholder", "patient_id": "system"}
                )
                db = Chroma.from_documents(
                    documents=[placeholder_doc],
                    embedding=EMBEDDINGS,
                    persist_directory=db_path,
                    collection_name="UserHistory",
                    collection_metadata={"hnsw:space": "cosine"}
                )
                logger.info(f"âœ… ç”¨æˆ·å†å²åº“åˆ›å»ºæˆåŠŸï¼ˆç©ºåº“ï¼‰")
    
    else:  # update æ¨¡å¼
        print(f"\nğŸ“¦ è¿è¡Œæ¨¡å¼: UPDATEï¼ˆå¢é‡æ›´æ–° + {chunker_name}ï¼‰\n")
        
        if args.db in ['all', 'guide']:
            update_vector_db("MedicalGuide_db", "MedicalGuide_data", use_dynamic_chunker=use_dynamic)
        
        if args.db in ['all', 'process']:
            update_vector_db("HospitalProcess_db", "HospitalProcess_data", use_dynamic_chunker=use_dynamic)
        
        if args.db in ['all', 'case']:
            update_vector_db("ClinicalCase_db", "ClinicalCase_data", use_dynamic_chunker=use_dynamic)
        
        if args.db in ['all', 'qa']:
            init_high_quality_qa_db()
        
        if args.db == 'history':
            logger.info("âš ï¸ ç”¨æˆ·å†å²åº“ç”±è¿è¡Œæ—¶åŠ¨æ€æ›´æ–°ï¼Œæ— éœ€æ‰‹åŠ¨åˆå§‹åŒ–")


if __name__ == "__main__":
    main()