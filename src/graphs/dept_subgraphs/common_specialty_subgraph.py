"""é€šç”¨ä¸“ç§‘å­å›¾ï¼šæ”¯æŒæ‰€æœ‰ç§‘å®¤çš„ä¸“ç§‘é—®è¯Šã€ä½“æ£€ã€åˆæ­¥åˆ¤æ–­"""
from __future__ import annotations

import json
import random
from typing import Any

from langgraph.graph import END, StateGraph

from rag import ChromaRetriever
from services.llm_client import LLMClient
from state.schema import BaseState, make_audit_entry
from utils import load_prompt, contains_any_positive, get_logger
from environment.staff_tracker import StaffTracker  # å¯¼å…¥åŒ»æŠ¤äººå‘˜çŠ¶æ€è¿½è¸ªå™¨
from output_config import should_log, OutputFilter, SUPPRESS_UNCHECKED_LOGS  # å¯¼å…¥è¾“å‡ºé…ç½®

# åˆå§‹åŒ–logger
logger = get_logger("hospital_agent.specialty_subgraph")

# åº”ç”¨è¾“å‡ºè¿‡æ»¤å™¨æ¥æŠ‘åˆ¶æœªè¢«should_logåŒ…è£…çš„æ—¥å¿—
if SUPPRESS_UNCHECKED_LOGS:
    logger.addFilter(OutputFilter("specialty_subgraph"))


# Typeæ ‡å‡†åŒ–æ˜ å°„å¸¸é‡ï¼ˆå°†å„ç§å˜ä½“æ˜ å°„åˆ°æ ‡å‡†typeï¼‰
TEST_TYPE_MAPPING = {
    "è¡€æ¶²æ£€æŸ¥": "lab",
    "è¡€æ¶²": "lab",
    "æ£€éªŒ": "lab",
    "å®éªŒå®¤": "lab",
    "åŒ–éªŒ": "lab",
    "å°¿æ¶²æ£€æŸ¥": "lab",
    "å¤§ä¾¿æ£€æŸ¥": "lab",
    "å…ç–«å­¦æ£€æŸ¥": "lab",
    "ç‚ç—‡æ ‡å¿—ç‰©": "lab",
    "è¡€æ¸…å­¦æ£€æŸ¥": "lab",
    "å½±åƒæ£€æŸ¥": "imaging",
    "å½±åƒ": "imaging",
    "æ”¾å°„": "imaging",
    "è¶…å£°": "imaging",
    "å†…é•œæ£€æŸ¥": "endoscopy",
    "å†…é•œ": "endoscopy",
    "é•œæ£€": "endoscopy",
    "åŠŸèƒ½æ£€æŸ¥": "neurophysiology",
    "ç”µç”Ÿç†": "neurophysiology",
    "ç¥ç»ç”µç”Ÿç†": "neurophysiology",
}


def _validate_and_normalize_test(test: dict[str, Any], dept: str, dept_config: dict) -> dict[str, Any] | None:
    """
    æ ‡å‡†åŒ–æ£€æŸ¥é¡¹ç›®ï¼ˆä¸åšç™½åå•æ ¡éªŒï¼Œå®Œå…¨ä¿¡ä»»LLMåˆ¤æ–­ï¼‰
    
    Args:
        test: åŸå§‹æ£€æŸ¥é¡¹ç›®
        dept: ç§‘å®¤ä»£ç 
        dept_config: ç§‘å®¤é…ç½®
        
    Returns:
        æ ‡å‡†åŒ–åçš„æ£€æŸ¥é¡¹ç›®
    """
    test_name = str(test.get("name", "")).strip()
    test_type = str(test.get("type", "lab")).lower()
    
    if not test_name:
        logger.warning(f"  âš ï¸  æ£€æŸ¥é¡¹ç›®åç§°ä¸ºç©ºï¼Œè·³è¿‡")
        return None
    
    # å¦‚æœtypeä¸æ˜¯æ ‡å‡†å€¼ï¼Œå°è¯•æ˜ å°„
    if test_type not in ["lab", "imaging", "endoscopy", "neurophysiology"]:
        test_type = TEST_TYPE_MAPPING.get(test_type, "lab")  # é»˜è®¤ä¸ºlab
        logger.debug(f"  ğŸ”„ æ£€æŸ¥ç±»å‹æ ‡å‡†åŒ–: {test.get('type')} â†’ {test_type}")
    
    # è·å–æ£€æŸ¥éƒ¨ä½ï¼ˆå¦‚æœé…ç½®äº†ï¼‰
    test_body_parts = dept_config.get("test_body_parts", {})
    body_part = test_body_parts.get(test_name, ["ç›¸å…³éƒ¨ä½"])
    
    return {
        "dept": dept,
        "type": test_type,
        "name": test_name,
        "reason": test.get("reason", "è¿›ä¸€æ­¥æ˜ç¡®è¯Šæ–­"),
        "priority": test.get("priority", "routine"),
        "need_prep": bool(test.get("need_prep", test_type in ["endoscopy"])),
        "need_schedule": bool(test.get("need_schedule", test_type in ["endoscopy", "neurophysiology"])),
        "body_part": body_part,
    }


def _chunks_for_prompt(chunks: list[dict[str, Any]], *, max_chars: int = 1400) -> str:
    lines: list[str] = []
    total = 0
    for c in chunks:
        text = str(c.get("text") or "").replace("\n", " ").strip()
        line = f"[{c.get('doc_id')}#{c.get('chunk_id')}] {text[:240]}"
        lines.append(line)
        total += len(line) + 1
        if total >= max_chars:
            break
    return "\n".join(lines)


# ç§‘å®¤é…ç½®æ˜ å°„ï¼ˆ15ä¸ªæ ‡å‡†ç§‘å®¤ï¼‰
DEPT_CONFIG = {
    "internal_medicine": {
        "name": "å†…ç§‘",
        "interview_keys": ["symptom_detail", "duration", "severity", "related_factors", "alarm_symptoms"],
        "alarm_keywords": ["é«˜çƒ­ä¸é€€", "ä¸¥é‡èƒ¸ç—›", "å‘¼å¸å›°éš¾", "æ„è¯†æ”¹å˜", "å‰§çƒˆè…¹ç—›"],
        "exam_area": "general_internal",
        "common_tests": ["è¡€å¸¸è§„", "å°¿å¸¸è§„", "è‚åŠŸèƒ½", "è‚¾åŠŸèƒ½", "å¿ƒç”µå›¾", "èƒ¸ç‰‡"],
        "allowed_tests": {
            "lab": ["è¡€å¸¸è§„", "å°¿å¸¸è§„", "å¤§ä¾¿å¸¸è§„", "è‚åŠŸèƒ½", "è‚¾åŠŸèƒ½", "ç”µè§£è´¨", "è¡€ç³–", "è¡€è„‚", "ç”²çŠ¶è…ºåŠŸèƒ½", "å¿ƒè‚Œé…¶"],
            "imaging": ["èƒ¸ç‰‡", "è…¹éƒ¨Bè¶…", "å¿ƒè„å½©è¶…", "èƒ¸éƒ¨CT", "è…¹éƒ¨CT"],
            "endoscopy": ["èƒƒé•œ", "è‚ é•œ"],
            "neurophysiology": []
        },
        "test_body_parts": {
            "èƒ¸ç‰‡": ["èƒ¸éƒ¨"],
            "è…¹éƒ¨Bè¶…": ["è…¹éƒ¨"],
            "å¿ƒè„å½©è¶…": ["å¿ƒè„"],
            "èƒ¸éƒ¨CT": ["èƒ¸éƒ¨"],
            "è…¹éƒ¨CT": ["è…¹éƒ¨"],
            "èƒƒé•œ": ["ä¸Šæ¶ˆåŒ–é“"],
            "è‚ é•œ": ["ä¸‹æ¶ˆåŒ–é“"]
        },
    },
    "surgery": {
        "name": "å¤–ç§‘",
        "interview_keys": ["injury_mechanism", "wound_status", "pain_level", "bleeding_status"],
        "alarm_keywords": ["å¤§å‡ºè¡€", "å¼€æ”¾æ€§éª¨æŠ˜", "è…¹è†œåˆºæ¿€å¾", "è„å™¨æŸä¼¤"],
        "exam_area": "surgical",
        "common_tests": ["Xçº¿", "CT", "Bè¶…", "è¡€å¸¸è§„"],
        "allowed_tests": {
            "lab": ["è¡€å¸¸è§„", "å‡è¡€åŠŸèƒ½", "è‚åŠŸèƒ½", "è‚¾åŠŸèƒ½"],
            "imaging": ["Xçº¿", "CT", "Bè¶…", "MRI"],
            "endoscopy": [],
            "neurophysiology": []
        },
        "test_body_parts": {
            "Xçº¿": ["éª¨éª¼", "å…³èŠ‚", "èƒ¸éƒ¨", "è…¹éƒ¨"],
            "CT": ["å…¨èº«å„éƒ¨ä½"],
            "Bè¶…": ["è…¹éƒ¨", "è½¯ç»„ç»‡"],
            "MRI": ["å…¨èº«å„éƒ¨ä½"]
        },
    },
    "orthopedics": {
        "name": "éª¨ç§‘",
        "interview_keys": ["injury_mechanism", "joint_function", "pain_pattern", "mobility"],
        "alarm_keywords": ["éª¨æŠ˜", "å…³èŠ‚è„±ä½", "ç¥ç»æŸä¼¤", "è¡€ç®¡æŸä¼¤"],
        "exam_area": "musculoskeletal",
        "common_tests": ["Xçº¿", "CT", "MRI", "éª¨å¯†åº¦"],
        "allowed_tests": {
            "lab": ["è¡€å¸¸è§„", "è¡€æ²‰", "CRP", "ç±»é£æ¹¿å› å­"],
            "imaging": ["Xçº¿", "CT", "MRI", "éª¨å¯†åº¦", "å…³èŠ‚Bè¶…"],
            "endoscopy": ["å…³èŠ‚é•œ"],
            "neurophysiology": ["è‚Œç”µå›¾"]
        },
        "test_body_parts": {
            "Xçº¿": ["éª¨éª¼", "å…³èŠ‚"],
            "CT": ["éª¨éª¼", "å…³èŠ‚"],
            "MRI": ["éª¨éª¼", "å…³èŠ‚", "è½¯ç»„ç»‡"],
            "å…³èŠ‚é•œ": ["å…³èŠ‚è…”"]
        },
    },
    "urology": {
        "name": "æ³Œå°¿å¤–ç§‘",
        "interview_keys": ["urination_pattern", "hematuria_detail", "pain_location", "stone_history"],
        "alarm_keywords": ["æ— å°¿", "è¡€å°¿", "å‰§çƒˆè‚¾ç»ç—›", "å°¿æ½´ç•™"],
        "exam_area": "urogenital",
        "common_tests": ["æ³Œå°¿ç³»Bè¶…", "CTæ³Œå°¿ç³»é€ å½±", "å°¿å¸¸è§„", "è‚¾åŠŸèƒ½"],
        "allowed_tests": {
            "lab": ["å°¿å¸¸è§„", "è‚¾åŠŸèƒ½", "å‰åˆ—è…ºç‰¹å¼‚æŠ—åŸ"],
            "imaging": ["æ³Œå°¿ç³»Bè¶…", "CTæ³Œå°¿ç³»é€ å½±", "IVP", "è†€èƒ±é•œ"],
            "endoscopy": ["è†€èƒ±é•œ", "è¾“å°¿ç®¡é•œ"],
            "neurophysiology": []
        },
    },
    "obstetrics_gynecology": {
        "name": "å¦‡äº§ç§‘",
        "interview_keys": ["menstrual_history", "pregnancy_status", "vaginal_discharge", "pain_location"],
        "alarm_keywords": ["é˜´é“å¤§å‡ºè¡€", "å‰§çƒˆè…¹ç—›", "å…ˆå…†æµäº§", "å®«å¤–å­•"],
        "exam_area": "gynecological",
        "common_tests": ["å¦‡ç§‘Bè¶…", "HCG", "å¦‡ç§‘æ£€æŸ¥", "å®«é¢ˆæ¶‚ç‰‡"],
        "allowed_tests": {
            "lab": ["HCG", "æ€§æ¿€ç´ ", "ç™½å¸¦å¸¸è§„", "å®«é¢ˆæ¶‚ç‰‡"],
            "imaging": ["å¦‡ç§‘Bè¶…", "ç›†è…”MRI"],
            "endoscopy": ["é˜´é“é•œ", "å®«è…”é•œ", "è…¹è…”é•œ"],
            "neurophysiology": []
        },
    },
    "pediatrics": {
        "name": "å„¿ç§‘",
        "interview_keys": ["age", "growth_development", "feeding_pattern", "vaccination_history"],
        "alarm_keywords": ["é«˜çƒ­æƒŠå¥", "å‘¼å¸å›°éš¾", "è„±æ°´", "å‘è‚²è¿Ÿç¼“"],
        "exam_area": "pediatric",
        "common_tests": ["è¡€å¸¸è§„", "èƒ¸ç‰‡", "å‘è‚²è¯„ä¼°", "è¿‡æ•åŸæ£€æµ‹"],
        "allowed_tests": {
            "lab": ["è¡€å¸¸è§„", "è¿‡æ•åŸæ£€æµ‹", "å¾®é‡å…ƒç´ ", "éª¨é¾„"],
            "imaging": ["èƒ¸ç‰‡", "Bè¶…"],
            "endoscopy": [],
            "neurophysiology": []
        },
    },
    "neurology": {
        "name": "ç¥ç»åŒ»å­¦",
        "interview_keys": ["onset_time", "frequency", "severity", "triggers", "relievers", "red_flags"],
        "alarm_keywords": ["çªå‘", "åç˜«", "è‚¢ä½“æ— åŠ›", "è¨€è¯­ä¸æ¸…", "æ„è¯†éšœç¢", "æŠ½æ"],
        "exam_area": "neurological",
        "common_tests": ["å¤´é¢…CT", "å¤´é¢…MRI", "è„‘ç”µå›¾", "è‚Œç”µå›¾"],
    },
    "oncology": {
        "name": "è‚¿ç˜¤ç§‘",
        "interview_keys": ["tumor_history", "treatment_history", "current_symptoms", "metastasis"],
        "alarm_keywords": ["æ¶æ€§è‚¿ç˜¤", "è½¬ç§»", "ç—…ç†æ€§éª¨æŠ˜", "ä¸Šè…”é™è„‰ç»¼åˆå¾"],
        "exam_area": "oncological",
        "common_tests": ["è‚¿ç˜¤æ ‡å¿—ç‰©", "PET-CT", "ç—…ç†æ´»æ£€", "å…¨èº«éª¨æ‰«æ"],
        "allowed_tests": {
            "lab": ["è‚¿ç˜¤æ ‡å¿—ç‰©", "è¡€å¸¸è§„", "è‚è‚¾åŠŸèƒ½"],
            "imaging": ["PET-CT", "å¢å¼ºCT", "å¢å¼ºMRI", "å…¨èº«éª¨æ‰«æ"],
            "endoscopy": ["ç—…ç†æ´»æ£€"],
            "neurophysiology": []
        },
    },
    "infectious_disease": {
        "name": "æ„ŸæŸ“æ€§ç–¾ç—…ç§‘",
        "interview_keys": ["fever_pattern", "exposure_history", "travel_history", "contact_history"],
        "alarm_keywords": ["é«˜çƒ­ä¸é€€", "è„“æ¯’ç—‡", "ä¼ æŸ“ç—…æ¥è§¦å²", "å…ç–«ç¼ºé™·"],
        "exam_area": "infectious",
        "common_tests": ["è¡€åŸ¹å…»", "ç—…åŸå­¦æ£€æµ‹", "è‚åŠŸèƒ½", "HIVæ£€æµ‹"],
        "allowed_tests": {
            "lab": ["è¡€åŸ¹å…»", "ç—…åŸå­¦æ£€æµ‹", "è‚åŠŸèƒ½", "HIVæ£€æµ‹", "è¡€å¸¸è§„", "CRP"],
            "imaging": ["èƒ¸ç‰‡", "CT"],
            "endoscopy": [],
            "neurophysiology": []
        },
    },
    "dermatology_std": {
        "name": "çš®è‚¤æ€§ç—…ç§‘",
        "interview_keys": ["rash_distribution", "itching_severity", "sexual_history", "skin_lesion"],
        "alarm_keywords": ["å…¨èº«æ€§çš®ç–¹", "ä¸¥é‡è¿‡æ•", "æ€§ç—…å²", "çš®è‚¤æ„ŸæŸ“"],
        "exam_area": "dermatological",
        "common_tests": ["çš®è‚¤é•œæ£€", "è¿‡æ•åŸæ£€æµ‹", "æ€§ç—…ç­›æŸ¥", "çœŸèŒåŸ¹å…»"],
        "allowed_tests": {
            "lab": ["è¿‡æ•åŸæ£€æµ‹", "æ€§ç—…ç­›æŸ¥", "çœŸèŒåŸ¹å…»"],
            "imaging": [],
            "endoscopy": ["çš®è‚¤é•œæ£€", "çš®è‚¤æ´»æ£€"],
            "neurophysiology": []
        },
    },
    "ent_ophthalmology_stomatology": {
        "name": "çœ¼è€³é¼»å–‰å£è…”ç§‘",
        "interview_keys": ["affected_organ", "vision_hearing_changes", "pain_level", "discharge"],
        "alarm_keywords": ["æ€¥æ€§è§†åŠ›ä¸‹é™", "çªå‘æ€§è€³è‹", "å‘¼å¸é“æ¢—é˜»", "ä¸¥é‡å¤–ä¼¤"],
        "exam_area": "ent_ophthal",
        "common_tests": ["è§†åŠ›æ£€æŸ¥", "å¬åŠ›æ£€æŸ¥", "é¼»å’½é•œ", "å£è…”æ£€æŸ¥"],
        "allowed_tests": {
            "lab": [],
            "imaging": ["CT", "MRI"],
            "endoscopy": ["é¼»å’½é•œ", "å–‰é•œ", "è€³å†…é•œ"],
            "neurophysiology": ["å¬åŠ›æ£€æŸ¥", "è§†åŠ›æ£€æŸ¥"]
        },
    },
    "psychiatry": {
        "name": "ç²¾ç¥å¿ƒç†ç§‘",
        "interview_keys": ["mood_changes", "sleep_pattern", "suicidal_ideation", "psychotic_symptoms"],
        "alarm_keywords": ["è‡ªæ€å€¾å‘", "ä¼¤äººå€¾å‘", "ä¸¥é‡å¹»è§‰", "ä¸¥é‡å¦„æƒ³"],
        "exam_area": "psychiatric",
        "common_tests": ["å¿ƒç†é‡è¡¨", "ç²¾ç¥çŠ¶æ€æ£€æŸ¥", "è®¤çŸ¥åŠŸèƒ½è¯„ä¼°"],
        "allowed_tests": {
            "lab": [],
            "imaging": [],
            "endoscopy": [],
            "neurophysiology": ["å¿ƒç†é‡è¡¨", "è®¤çŸ¥åŠŸèƒ½è¯„ä¼°"]
        },
    },
    "emergency": {
        "name": "æ€¥è¯ŠåŒ»å­¦ç§‘",
        "interview_keys": ["onset_time", "severity", "vital_signs", "trauma_mechanism"],
        "alarm_keywords": ["ä¼‘å…‹", "å¿ƒè·³éª¤åœ", "å¤§å‡ºè¡€", "ä¸¥é‡åˆ›ä¼¤", "ä¸­æ¯’", "çª’æ¯"],
        "exam_area": "emergency",
        "common_tests": ["è¡€æ°”åˆ†æ", "å¿ƒç”µå›¾", "å¿«é€ŸåºŠæ—æ£€æŸ¥", "Xçº¿"],
        "allowed_tests": {
            "lab": ["è¡€æ°”åˆ†æ", "è¡€å¸¸è§„", "å‡è¡€åŠŸèƒ½", "å¿ƒè‚Œé…¶"],
            "imaging": ["Xçº¿", "CT", "Bè¶…"],
            "endoscopy": [],
            "neurophysiology": ["å¿ƒç”µå›¾"]
        },
    },
    "rehabilitation_pain": {
        "name": "åº·å¤ç–¼ç—›ç§‘",
        "interview_keys": ["pain_duration", "pain_character", "functional_limitation", "treatment_history"],
        "alarm_keywords": ["ç¥ç»ç—…ç†æ€§ç–¼ç—›", "ç™Œæ€§ç–¼ç—›", "å¤æ‚åŒºåŸŸç–¼ç—›ç»¼åˆå¾"],
        "exam_area": "rehabilitation",
        "common_tests": ["åŠŸèƒ½è¯„ä¼°", "ç–¼ç—›è¯„åˆ†", "è‚Œç”µå›¾", "å½±åƒå­¦æ£€æŸ¥"],
        "allowed_tests": {
            "lab": [],
            "imaging": ["Xçº¿", "MRI"],
            "endoscopy": [],
            "neurophysiology": ["è‚Œç”µå›¾", "åŠŸèƒ½è¯„ä¼°"]
        },
    },
    "traditional_chinese_medicine": {
        "name": "ä¸­åŒ»ç§‘",
        "interview_keys": ["tcm_syndrome", "tongue_pulse", "constitution", "lifestyle"],
        "alarm_keywords": ["æ€¥å±é‡ç—‡", "éœ€è¥¿åŒ»æ€¥æ•‘"],
        "exam_area": "tcm",
        "common_tests": ["ä¸­åŒ»ä½“è´¨è¾¨è¯†", "èˆŒè¯Š", "è„‰è¯Š", "ç»ç»œæ£€æµ‹"],
        "allowed_tests": {
            "lab": [],
            "imaging": [],
            "endoscopy": [],
            "neurophysiology": ["ä¸­åŒ»ä½“è´¨è¾¨è¯†", "ç»ç»œæ£€æµ‹"]
        },
    },
}


def build_common_specialty_subgraph(
    *, 
    retriever: ChromaRetriever,
    llm: LLMClient | None = None,
    doctor_agent=None, 
    patient_agent=None, 
    max_questions: int = 3  # æœ€åº•å±‚é»˜è®¤å€¼ï¼Œé€šå¸¸ä»config.yamlä¼ å…¥
):
    """æ„å»ºé€šç”¨ä¸“ç§‘å­å›¾ï¼Œé€‚ç”¨äºæ‰€æœ‰ç§‘å®¤
    
    Args:
        max_questions: åŒ»ç”Ÿæœ€å¤šé—®è¯Šæ¬¡æ•°ï¼ˆä»config.agent.max_questionsä¼ å…¥ï¼‰
    """
    graph = StateGraph(BaseState)
    
    # åˆ¤æ–­æ˜¯å¦å¯ç”¨Agentæ¨¡å¼
    use_agents = doctor_agent is not None and patient_agent is not None

    def s4_specialty_interview(state: BaseState) -> BaseState:
        """S4: é€šç”¨ä¸“ç§‘é—®è¯ŠèŠ‚ç‚¹"""
        dept = state.dept
        dept_config = DEPT_CONFIG.get(dept, DEPT_CONFIG.get("internal_medicine", {}))
        dept_name = dept_config.get("name", "é€šç”¨ç§‘å®¤")
        
        # ç»ˆç«¯ç®€æ´è¾“å‡º
        if should_log(1, "specialty_subgraph", "S4"):
            logger.info(f"ğŸ« S4: {dept_name}ä¸“ç§‘é—®è¯Š")
        
        # è¯¦ç»†æ—¥å¿—è®°å½•
        detail_logger = state.patient_detail_logger if hasattr(state, 'patient_detail_logger') else None
        if detail_logger:
            detail_logger.section(f"{dept_name}ä¸“ç§‘é—®è¯Š")
        
        # å¦‚æœæ˜¯Agentæ¨¡å¼ï¼Œç¡®ä¿åŒ»ç”Ÿæ™ºèƒ½ä½“çš„ç§‘å®¤è®¾ç½®æ­£ç¡®
        if use_agents and doctor_agent:
            doctor_agent.dept = dept
            logger.info(f"  ğŸ‘¨â€âš•ï¸ è®¾ç½®åŒ»ç”Ÿä¸º{dept_name}ä¸“ç§‘åŒ»ç”Ÿ")
        
        # æ£€ç´¢è¯¥ç§‘å®¤çš„ä¸“ç§‘çŸ¥è¯†
        # æ³¨æ„ï¼šæ­¤æ—¶chief_complaintè¿˜æœªè®¾ç½®ï¼ˆåŒ»ç”Ÿå°šæœªä»æ‚£è€…å¤„è·å¾—ï¼‰ï¼Œä½¿ç”¨ç§‘å®¤ä¿¡æ¯æ£€ç´¢
        query = f"{dept} {dept_name} çº¢æ—— æ£€æŸ¥å»ºè®® é‰´åˆ«è¯Šæ–­"
        logger.info(f"ğŸ” æ£€ç´¢{dept_name}çŸ¥è¯†...")
        chunks = retriever.retrieve(query, filters={"dept": dept}, k=4)
        state.add_retrieved_chunks(chunks)
        logger.info(f"  âœ… æ£€ç´¢åˆ° {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

        cc = state.chief_complaint
        
        # è·å–ç§‘å®¤é…ç½®ç”¨äºæç¤ºè¯
        alarm_keywords = dept_config.get("alarm_keywords", [])
        interview_keys = dept_config.get("interview_keys", ["symptoms_detail"])

        # è·å–èŠ‚ç‚¹ä¸“å±è®¡æ•°å™¨
        node_key = f"s4_{dept}"
        
        # Agentæ¨¡å¼ï¼šé€æ­¥ä¸€é—®ä¸€ç­”ï¼Œç„¶åä»doctor_agentæ”¶é›†ç»“æ„åŒ–ä¿¡æ¯
        if use_agents:
            # è·å–æœ€å¤§é—®è¯Šè½®æ•°ï¼ˆä¼˜å…ˆä½¿ç”¨state.agent_configï¼Œå…¶æ¬¡ä½¿ç”¨å‡½æ•°å‚æ•°ï¼‰
            # ç¡®ä¿ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®çš„å€¼ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„é»˜è®¤å€¼
            if state.agent_config and "max_questions" in state.agent_config:
                max_qs = state.agent_config["max_questions"]
            else:
                max_qs = max_questions  # ä½¿ç”¨å‡½æ•°å‚æ•°ï¼ˆæ¥è‡ªé…ç½®æ–‡ä»¶ï¼‰
            
            # å¼€å§‹é—®è¯Š
            logger.info(f"  ğŸ’¬ é—®è¯Šå¼€å§‹")
            
            if detail_logger:
                detail_logger.subsection("åŒ»ç”Ÿé—®è¯Š")
            
            # ===== ç‰©ç†ç¯å¢ƒé›†æˆï¼šé—®è¯Šå‰æ£€æŸ¥æ‚£è€…çŠ¶æ€ =====
            if state.world_context:
                impact = state.get_physical_impact_on_diagnosis()
                if impact.get("has_impact"):
                    logger.info("\n" + "="*60)
                    logger.info("âš ï¸  ç‰©ç†çŠ¶æ€å½±å“è¯Šæ–­")
                    logger.info("="*60)
                    
                    # æ˜¾ç¤ºä¸¥é‡è­¦å‘Š
                    warnings = impact.get("warnings", [])
                    if warnings:
                        for warning in warnings:
                            logger.warning(warning)
                    
                    # æ˜¾ç¤ºå»ºè®®
                    for suggestion in impact.get("suggestions", []):
                        logger.info(f"  ğŸ’¡ {suggestion}")
                    
                    logger.info("="*60)
                    
                    # æ ¹æ®ä½“åŠ›é™åˆ¶é—®è¯Šè½®æ•°
                    physical_max_questions = impact.get("max_questions", max_qs)
                    if physical_max_questions < max_qs:
                        logger.info(f"  âš™ï¸  æ ¹æ®æ‚£è€…çŠ¶æ€ï¼Œé—®è¯Šè½®æ•°è°ƒæ•´ä¸º {physical_max_questions}")
                        max_qs = physical_max_questions
                    
                    # å¦‚æœæ‚£è€…æ„è¯†å¼‚å¸¸ï¼Œæ ‡è®°ä¸ºç´§æ€¥
                    if impact.get("emergency"):
                        logger.error("  ğŸš¨ğŸš¨ ç´§æ€¥æƒ…å†µï¼šæ‚£è€…æ„è¯†å¼‚å¸¸ï¼Œå»ºè®®ç«‹å³è½¬æ€¥è¯Šï¼")
                        state.escalations.append("æ‚£è€…æ„è¯†å¼‚å¸¸ï¼Œå»ºè®®æ€¥è¯Šè¯„ä¼°")
                        # ä¸åº”ç»§ç»­å¸¸è§„é—®è¯Š
                        if max_qs > 0:
                            logger.warning("  âš ï¸  ç”±äºç´§æ€¥æƒ…å†µï¼Œè·³è¿‡å¸¸è§„é—®è¯Š")
                            max_qs = 0
            
            # ä½¿ç”¨å…¨å±€å…±äº«è®¡æ•°å™¨
            global_qa_count = state.node_qa_counts.get("global_total", 0)
            questions_asked_this_node = state.node_qa_counts.get(node_key, 0)
            
            # è®¡ç®—æœ¬èŠ‚ç‚¹å‰©ä½™é—®é¢˜æ•°ï¼šæœ¬èŠ‚ç‚¹é…é¢ - æœ¬èŠ‚ç‚¹å·²é—®æ•°
            # ä¸ä½¿ç”¨å…¨å±€è®¡æ•°å™¨é™åˆ¶ï¼Œå› ä¸ºæ¯ä¸ªä¸“ç§‘èŠ‚ç‚¹åº”è¯¥æœ‰ç‹¬ç«‹çš„é—®è¯Šæœºä¼š
            remaining_questions = max(0, max_qs - questions_asked_this_node)
            
            if detail_logger:
                detail_logger.info(f"å…¨å±€å·²é—® {global_qa_count} ä¸ªï¼Œæœ¬èŠ‚ç‚¹å·²é—® {questions_asked_this_node} ä¸ªï¼Œæœ¬èŠ‚ç‚¹å‰©ä½™ {remaining_questions} ä¸ª")
            
            # é€ä¸ªç”Ÿæˆé—®é¢˜å¹¶è·å–å›ç­”
            qa_list = state.agent_interactions.get("doctor_patient_qa", [])
            
            # è·å–æ‚£è€…è¯¦ç»†æ—¥å¿—è®°å½•å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            detail_logger = state.patient_detail_logger if hasattr(state, 'patient_detail_logger') else None
            
            for i in range(remaining_questions):
                # ç»ˆç«¯åªæ˜¾ç¤ºç®€æ´ä¿¡æ¯
                if should_log(1, "specialty_subgraph", "S4"):
                    logger.info(f"  ğŸ’¬ é—®è¯Šç¬¬ {questions_asked_this_node + i + 1} è½®")
                
                # åŒ»ç”ŸåŸºäºå½“å‰ä¿¡æ¯ç”Ÿæˆä¸€ä¸ªé—®é¢˜
                context_desc = f"{dept_name}ä¸“ç§‘é—®è¯Šï¼Œå…³æ³¨ï¼š{', '.join(interview_keys)}"
                if alarm_keywords:
                    context_desc += f"ï¼Œè­¦æŠ¥ç—‡çŠ¶ï¼š{', '.join(alarm_keywords)}"
                
                # ç¬¬ä¸€ä¸ªé—®é¢˜ï¼šå¦‚æœchief_complaintä¸ºç©ºï¼Œå…ˆé—®æ‚£è€…ä¸»è¯‰æ˜¯ä»€ä¹ˆ
                if i == 0 and not state.chief_complaint and not doctor_agent.questions_asked:
                    question = "æ‚¨å¥½ï¼Œè¯·é—®æ‚¨å“ªé‡Œä¸èˆ’æœï¼Ÿä¸»è¦æ˜¯ä»€ä¹ˆç—‡çŠ¶ï¼Ÿ"
                else:
                    # ä½¿ç”¨æ”¶é›†åˆ°çš„ä¿¡æ¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰æˆ–è€…æ‚£è€…çš„æè¿°ç”Ÿæˆé—®é¢˜
                    # æ³¨æ„ï¼šä¸ä½¿ç”¨state.chief_complaintï¼Œå› ä¸ºå®ƒè¿˜æœªç¡®å®š
                    question = doctor_agent.generate_one_question(
                        chief_complaint=doctor_agent.collected_info.get("chief_complaint", ""),
                        context=context_desc,
                        rag_chunks=chunks
                    )
                
                if not question:
                    if should_log(1, "specialty_subgraph", "S4"):
                        logger.info("  â„¹ï¸  åŒ»ç”Ÿæå‰ç»“æŸé—®è¯Š")
                    if detail_logger:
                        detail_logger.info("åŒ»ç”Ÿåˆ¤æ–­ä¿¡æ¯å·²å……è¶³ï¼Œæå‰ç»“æŸé—®è¯Š")
                    break
                
                # æ‚£è€…å›ç­”ï¼ˆä¼ å…¥ç‰©ç†çŠ¶æ€ï¼‰
                physical_state = state.physical_state_snapshot if state.world_context else None
                answer = patient_agent.respond_to_doctor(question, physical_state=physical_state)
                
                # è¯¦ç»†æ—¥å¿—ï¼šè®°å½•å®Œæ•´çš„é—®è¯Šå¯¹è¯
                if detail_logger:
                    detail_logger.qa_round(questions_asked_this_node + i + 1, question, answer)
                
                # åŒ»ç”Ÿå¤„ç†å›ç­”
                doctor_agent.process_patient_answer(question, answer)
                
                # ã€é‡è¦ã€‘åŒæ­¥æ›´æ–°åŒ»ç”Ÿçš„å¯¹è¯å†å²è®°å½•ï¼ˆç”¨äºä¸‹æ¬¡ç”Ÿæˆé—®é¢˜æ—¶å‚è€ƒï¼‰
                doctor_agent.collected_info.setdefault("conversation_history", [])
                doctor_agent.collected_info["conversation_history"].append({
                    "question": question,
                    "answer": answer
                })
                
                # è®°å½•å¯¹è¯åˆ°state
                qa_list.append({
                    "question": question, 
                    "answer": answer, 
                    "stage": f"{dept}_specialty"
                })
                
                # æ›´æ–°è¯¥èŠ‚ç‚¹å’Œå…¨å±€è®¡æ•°å™¨
                state.node_qa_counts[node_key] = questions_asked_this_node + i + 1
                state.node_qa_counts["global_total"] = global_qa_count + i + 1
            
            state.agent_interactions["doctor_patient_qa"] = qa_list
            
            # ===== StaffTrackeré›†æˆï¼šåŒºç”Ÿä¸“ç§‘é—®è¯Šå·¥ä½œ =====
            if state.world_context:
                actual_questions = state.node_qa_counts.get(node_key, 0) - questions_asked_this_node
                if actual_questions > 0:
                    # æ¯è½®é—®è¯Šçº¦2-3åˆ†é’Ÿ
                    consultation_time = actual_questions * 2.5
                    StaffTracker.update_doctor_consultation(
                        world=state.world_context,
                        duration_minutes=int(consultation_time),
                        complexity=0.6  # ä¸“ç§‘é—®è¯Šå¤æ‚åº¦ä¸­ç­‰åä¸Š
                    )
                    logger.info(f"  ğŸ‘¨â€âš•ï¸  åŒ»ç”Ÿå®Œæˆ{dept_name}ä¸“ç§‘é—®è¯Šï¼ˆ{actual_questions}è½®ï¼Œè€—æ—¶{int(consultation_time)}åˆ†é’Ÿï¼‰")
            
            # ===== ç‰©ç†ç¯å¢ƒé›†æˆï¼šé—®è¯Šåæ›´æ–°ç‰©ç†çŠ¶æ€ =====
            if state.world_context:
                qa_count = len([qa for qa in qa_list if qa.get('stage') == f"{dept}_specialty"])
                if qa_count > 0:
                    duration = qa_count * 3  # æ¯è½®çº¦3åˆ†é’Ÿ
                    energy_cost = 0.5 * qa_count  # æ¯è½®æ¶ˆè€—0.5ä½“åŠ›
                    
                    logger.info(f"\n{'â”€'*60}")
                    logger.info(f"ğŸŒ ç‰©ç†ç¯å¢ƒæ¨¡æ‹Ÿ - é—®è¯Šè¿‡ç¨‹")
                    logger.info(f"{'â”€'*60}")
                    start_time = state.world_context.current_time.strftime('%H:%M')
                    
                    result = state.update_physical_world(
                        action="consult",
                        duration_minutes=duration,
                        energy_cost=energy_cost
                    )
                    end_time = state.world_context.current_time.strftime('%H:%M')
                    
                    logger.info(f"ğŸ’¬ é—®è¯Šè½®æ•°: {qa_count}è½®")
                    logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration}åˆ†é’Ÿ")
                    logger.info(f"ğŸ• æ—¶é—´: {start_time} â†’ {end_time}")
                    logger.info(f"ğŸ’ª ä½“åŠ›: {result['physical_state']['energy_level']:.1f}/10 {'ğŸŸ¢' if result['physical_state']['energy_level'] > 7 else 'ğŸŸ¡' if result['physical_state']['energy_level'] > 4 else 'ğŸ”´'}")
                    logger.info(f"ğŸ˜£ ç–¼ç—›: {result['physical_state']['pain_level']:.1f}/10 {'ğŸŸ¢' if result['physical_state']['pain_level'] < 3 else 'ğŸŸ¡' if result['physical_state']['pain_level'] < 6 else 'ğŸ”´'}")
                    logger.info(f"{'â”€'*60}")
                    
                    # å¦‚æœå‡ºç°å±æ€¥è­¦æŠ¥
                    if result.get("critical_warning"):
                        logger.warning(f"ğŸš¨ è­¦å‘Šï¼šæ‚£è€…å‡ºç°å±æ€¥çŠ¶æ€ (æ„è¯†: {result.get('consciousness')})")
            
            # ä»åŒ»ç”Ÿæ”¶é›†çš„ä¿¡æ¯æ›´æ–°state
            state.history.update(doctor_agent.collected_info.get("history", {}))
            
            final_qa_count = state.node_qa_counts.get(node_key, 0)
            final_global_count = state.node_qa_counts.get("global_total", 0)
            logger.info(f"  âœ… {dept_name}ä¸“ç§‘é—®è¯Šå®Œæˆï¼Œæœ¬èŠ‚ç‚¹ {final_qa_count} è½®ï¼Œå…¨å±€æ€»è®¡ {final_global_count} è½®")
            
            # ===== åŒ»ç”Ÿæ€»ç»“ä¸“ä¸šä¸»è¯‰ =====
            # æ€»æ˜¯è®©åŒ»ç”ŸåŸºäºé—®è¯Šæ€»ç»“ä¸“ä¸šä¸»è¯‰ï¼Œè¦†ç›–æ‚£è€…å‘æŠ¤å£«è¯´çš„å£è¯­åŒ–æè¿°
            summarized_cc = doctor_agent.summarize_chief_complaint()
            if summarized_cc:
                # ä¿å­˜åŸå§‹ä¸»è¯‰ï¼ˆæ‚£è€…å‘æŠ¤å£«è¯´çš„ï¼‰ä¾›å‚è€ƒ
                if state.chief_complaint and state.chief_complaint != summarized_cc:
                    state.original_chief_complaint = state.chief_complaint
                # æ›´æ–°ä¸ºåŒ»ç”Ÿæ€»ç»“çš„ä¸“ä¸šä¸»è¯‰
                state.chief_complaint = summarized_cc
                logger.info(f"\n  ğŸ“‹ åŒ»ç”Ÿæ€»ç»“ä¸»è¯‰ï¼ˆä¸“ä¸šç‰ˆï¼‰: {summarized_cc}")
            
            # ===== æ–°å¢ï¼šé—®è¯Šè´¨é‡è¯„ä¼° =====
            # åªæœ‰åœ¨å®é™…é—®äº†é—®é¢˜æ—¶æ‰æ˜¾ç¤ºè¯„ä¼°
            if len(doctor_agent.questions_asked) > 0:
                logger.info(f"\n{'â”'*60}")
                logger.info("ğŸ“Š é—®è¯Šè´¨é‡è¯„ä¼°")
                logger.info(f"{'â”'*60}")
                
                quality_report = doctor_agent.assess_interview_quality()
                
                # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
                logger.info(f"  ğŸ“ˆ ç»¼åˆè¯„åˆ†: {quality_report['overall_score']}/100")
                logger.info(f"     â€¢ å®Œæ•´æ€§: {quality_report['completeness_score']:.0f}/100")
                logger.info(f"     â€¢ æ·±åº¦: {quality_report['depth_score']:.0f}/100")
                logger.info(f"     â€¢ æ•ˆç‡: {quality_report['efficiency_score']:.0f}/100")
                
                if quality_report['warning']:
                    if quality_report['overall_score'] < 50:
                        logger.warning(f"  {quality_report['warning']}")
                    elif quality_report['overall_score'] < 70:
                        logger.info(f"  {quality_report['warning']}")
                    else:
                        logger.info(f"  {quality_report['warning']}")
                
                # æ˜¾ç¤ºç¼ºå¤±ä¿¡æ¯
                if quality_report['missing_areas']:
                    logger.info(f"\n  âŒ ç¼ºå¤±å…³é”®ä¿¡æ¯ ({len(quality_report['missing_areas'])}é¡¹):")
                    for area in quality_report['missing_areas']:
                        logger.info(f"     â€¢ {area}")
                
                # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
                if quality_report['suggestions']:
                    logger.info(f"\n  ğŸ’¡ æ”¹è¿›å»ºè®®:")
                    for suggestion in quality_report['suggestions'][:3]:  # æœ€å¤šæ˜¾ç¤º3æ¡
                        logger.info(f"     â€¢ {suggestion}")
                
                logger.info(f"{'â”'*60}\n")
                
                # ä¿å­˜è¯„ä¼°ç»“æœåˆ°state
                state.agent_interactions["interview_quality"] = quality_report
            
            # Agentæ¨¡å¼ï¼šç›´æ¥ä»åŒ»ç”Ÿæ™ºèƒ½ä½“è·å–ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¸å†ç”¨LLMé‡å¤æå–
            interview = doctor_agent.collected_info.get(f"{dept}_interview", {})
            if not interview:
                # å¦‚æœåŒ»ç”Ÿæ²¡æœ‰ç‰¹å®šç§‘å®¤ä¿¡æ¯ï¼Œä½¿ç”¨é€šç”¨history
                interview = {
                    "collected_from_agent": True,
                    "alarm_symptoms": [],  # Agentä¼šåœ¨å¯¹è¯ä¸­å¤„ç†è­¦æŠ¥ç—‡çŠ¶
                }
                # åªæ›´æ–°éè­¦æŠ¥ç—‡çŠ¶ç›¸å…³çš„å­—æ®µï¼ˆé¿å…å°†"ä¸è¯¦"å­—ç¬¦ä¸²èµ‹å€¼ç»™è­¦æŠ¥ç—‡çŠ¶å­—æ®µï¼‰
                for key in interview_keys:
                    if key not in ["alarm_symptoms", "red_flags"]:
                        interview[key] = doctor_agent.collected_info.get("history", {}).get(key, "ä¸è¯¦")
            
            # ä» Agent æ”¶é›†ä¿¡æ¯
            if detail_logger:
                detail_logger.info("\nä» Agentæ”¶é›†çš„ä¸“ç§‘ä¿¡æ¯å·²æ•´åˆ")
        
        # éAgentæ¨¡å¼ï¼šä½¿ç”¨LLMæå–ä¸“ç§‘ä¿¡æ¯
        else:
            # ä½¿ç”¨LLMæå–
            if detail_logger:
                detail_logger.subsection("ä½¿ç”¨LLMæå–ä¸“ç§‘ä¿¡æ¯")
            system_prompt = load_prompt("common_system.txt")
            
            # æ ¹æ®ç§‘å®¤é€‰æ‹©ä¸åŒçš„prompt
            specialty_prompt_file = f"{dept}_specialty.txt"
            try:
                specialty_prompt = load_prompt(specialty_prompt_file)
            except:
                specialty_prompt = f"è¯·æå–{dept_name}ç›¸å…³çš„ä¸“ç§‘ä¿¡æ¯ã€‚"
            
            # ç®€åŒ–çš„æç¤ºè¯
            user_prompt = (
                specialty_prompt
                + f"\n\nã€ä»»åŠ¡ã€‘ä»ç—…ä¾‹ä¸­æå–{dept_name}ä¸“ç§‘ç»“æ„åŒ–ä¿¡æ¯\n"
                + f"ã€å…³æ³¨ç‚¹ã€‘{', '.join(interview_keys)}\n"
                + f"ã€è­¦æŠ¥ç—‡çŠ¶ã€‘{', '.join(alarm_keywords)}\n\n"
                + f"ã€ç—…ä¾‹ã€‘{cc}\n\n"
                + "ã€å‚è€ƒçŸ¥è¯†ã€‘\n" + _chunks_for_prompt(chunks) + "\n\n"
                + f"ã€è¾“å‡ºã€‘JSONæ ¼å¼ï¼Œå­—æ®µå: {dept}_interviewï¼ŒåŒ…å«ä¸Šè¿°å…³æ³¨ç‚¹åŠalarm_symptomsåˆ—è¡¨"
            )
            
            obj, used_fallback, _raw = llm.generate_json(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                fallback=lambda: {f"{dept}_interview": {key: "ä¸è¯¦" for key in interview_keys} | {"alarm_symptoms": []}},
                temperature=0.2,
            )
            interview = dict(obj.get(f"{dept}_interview") or {})
            # æå–å®Œæˆ
            if detail_logger:
                detail_logger.info("ä¸“ç§‘ä¿¡æ¯æå–å®Œæˆ")

        state.dept_payload.setdefault(dept, {})
        state.dept_payload[dept]["interview"] = interview

        # ç»Ÿä¸€è­¦æŠ¥ç—‡çŠ¶æ£€æµ‹ï¼ˆä»LLMè¿”å›çš„interviewä¸­è·å–ï¼‰
        # å®‰å…¨åœ°æå–è­¦æŠ¥ç—‡çŠ¶ï¼Œæ£€æŸ¥ç±»å‹é¿å…å°†å­—ç¬¦ä¸²æ‹†åˆ†æˆå­—ç¬¦åˆ—è¡¨
        raw_alarms = interview.get("alarm_symptoms") or interview.get("red_flags") or []
        if isinstance(raw_alarms, list):
            alarm_list = [str(a) for a in raw_alarms if a]  # è¿‡æ»¤ç©ºå€¼
        elif isinstance(raw_alarms, str) and raw_alarms not in ["ä¸è¯¦", "æ— ", ""]:
            alarm_list = [raw_alarms]  # å•ä¸ªå­—ç¬¦ä¸²è½¬ä¸ºåˆ—è¡¨
        else:
            alarm_list = []  # å¿½ç•¥å…¶ä»–æ— æ•ˆå€¼
        
        if alarm_list:
            detail_logger.warning(f"âš ï¸  å‘ç°è­¦æŠ¥ç—‡çŠ¶: {', '.join(str(a) for a in alarm_list)}")
            # ç»ˆç«¯è¾“å‡ºï¼ˆéœ€è¦output level >= 2ï¼‰
            if should_log(2, "specialty_subgraph", "S4"):
                logger.warning(f"  âš ï¸  å‘ç°è­¦æŠ¥ç—‡çŠ¶: {', '.join(str(a) for a in alarm_list)}")

        # è®°å½•èŠ‚ç‚¹é—®ç­”è½®æ•°
        node_qa_turns = state.node_qa_counts.get(node_key, 0)
        
        state.add_audit(
            make_audit_entry(
                node_name=f"S4 {dept_name} Specialty Interview",
                inputs_summary={"chief_complaint": state.chief_complaint, "use_agents": use_agents, "dept": dept, "max_questions": max_questions},
                outputs_summary={"alarm_symptoms": alarm_list, "node_qa_turns": node_qa_turns},
                decision=f"å®Œæˆ{dept_name}ä¸“ç§‘é—®è¯Šï¼ˆæœ¬èŠ‚ç‚¹{node_qa_turns}è½®ï¼‰" + ("ï¼ˆAgentæ¨¡å¼ï¼‰" if use_agents else ("ï¼ˆLLMæ¨¡å¼ï¼‰" if not used_fallback else "ï¼ˆFallbackï¼‰")),
                chunks=chunks,
                flags=["AGENT_MODE"] if use_agents else (["LLM_PARSE_FALLBACK"] if used_fallback else ["LLM_USED"]),
            )
        )
        if should_log(1, "specialty_subgraph", "S4"):
            logger.info(f"  âœ… S4å®Œæˆ\n")
        return state

    def s5_physical_exam(state: BaseState) -> BaseState:
        """S5: é€šç”¨ä½“æ£€èŠ‚ç‚¹"""
        dept = state.dept
        dept_config = DEPT_CONFIG.get(dept, DEPT_CONFIG.get("internal_medicine", {}))
        dept_name = dept_config.get("name", "é€šç”¨")
        exam_area = dept_config.get("exam_area", "general")
        alarm_keywords = dept_config.get("alarm_keywords", [])
        
        # è·å–è¯¦ç»†æ—¥å¿—è®°å½•å™¨
        detail_logger = state.patient_detail_logger if hasattr(state, 'patient_detail_logger') else None
        
        if should_log(1, "specialty_subgraph", "S5"):
            logger.info(f"ğŸ” S5: {dept_name}ä½“æ ¼æ£€æŸ¥")
        
        if detail_logger:
            detail_logger.section(f"{dept_name}ä½“æ ¼æ£€æŸ¥")
        
        # å½“å‰æ•°æ®æºåªæœ‰case_characterï¼Œä½¿ç”¨LLMç”Ÿæˆä½“æ£€ç»“æœ
        data_source = "llm_generated"
        real_physical_exam = None  # æ•°æ®é›†ä¸­æ²¡æœ‰ä½“æ ¼æ£€æŸ¥æ•°æ®
        
        logger.info(f"ğŸ“‹ ä½¿ç”¨LLMç”Ÿæˆä½“æ£€ç»“æœ")
        
        # ç»Ÿä¸€ç»“æ„åŒ–å¤„ç†æµç¨‹
        system_prompt = load_prompt("common_system.txt")
        
        # LLMç”Ÿæˆï¼šåŸºäºä¸»è¯‰å’Œä¸“ç§‘ä¿¡æ¯
        interview_info = state.dept_payload.get(dept, {}).get("interview", {})
        interview_str = json.dumps(interview_info, ensure_ascii=False) if interview_info else "æ— "
        
        user_prompt = (
                f"æ ¹æ®{dept_name}ç§‘å®¤ç‰¹ç‚¹ï¼Œç”Ÿæˆåˆç†çš„ä½“æ ¼æ£€æŸ¥ç»“æœã€‚\n\n"
                + f"ã€ä¸»è¯‰ã€‘{state.chief_complaint}\n"
                + f"ã€ä¸“ç§‘é—®è¯Šã€‘{interview_str}\n\n"
                + f"ã€è¦æ±‚ã€‘\n"
                + f"1. åŒ…å«vital_signsï¼ˆç”Ÿå‘½ä½“å¾ï¼‰å’Œgeneralï¼ˆä¸€èˆ¬æƒ…å†µï¼‰\n"
                + f"2. æ ¹æ®{exam_area}æ·»åŠ ä¸“ç§‘ä½“æ£€é¡¹ç›®\n"
                + f"3. ç»“æœåº”ä¸ä¸»è¯‰ç›¸ç¬¦ï¼Œè€ƒè™‘è­¦æŠ¥ç—‡çŠ¶ï¼š{', '.join(alarm_keywords)}\n\n"
                + "ã€è¾“å‡ºã€‘JSONæ ¼å¼ï¼š{\"exam\": {...}}"
        )
        fallback_data = {
            "exam": {
                "vital_signs": {"temperature": "æ­£å¸¸", "pulse": "æ­£å¸¸", "blood_pressure": "æ­£å¸¸"},
                "general": "ä¸€èˆ¬æƒ…å†µå¯",
                "note": f"{dept_name}ä½“æ ¼æ£€æŸ¥"
            }
        }
        temp = 0.2
        
        # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
        if llm is None:
            logger.error("âš ï¸  æœªLLMé…ç½®ï¼Œæ— æ³•ç”Ÿæˆä½“æ ¼æ£€æŸ¥ç»“æœ")
            exam = fallback_data["exam"]
            exam["source"] = "no_llm"
            used_fallback = True
        else:
            # æ‰§è¡ŒLLMè°ƒç”¨
            obj, used_fallback, _raw = llm.generate_json(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                fallback=lambda: fallback_data,
                temperature=temp,
            )
            exam = dict(obj.get("exam") or {})
            exam["source"] = data_source
            logger.info("  âœ… ä½“æ ¼æ£€æŸ¥å¤„ç†å®Œæˆ")
        
        state.exam_findings.setdefault(exam_area, {})
        state.exam_findings[exam_area] = exam

        state.add_audit(
            make_audit_entry(
                node_name=f"S5 {dept_name} Physical Exam",
                inputs_summary={"exam_area": exam_area, "dept": dept, "has_real_data": bool(real_physical_exam)},
                outputs_summary={"exam_completed": True, "data_source": exam.get("source", "unknown")},
                decision=f"å®Œæˆ{dept_name}ä½“æ ¼æ£€æŸ¥è®°å½•" + ("ï¼ˆä½¿ç”¨æ•°æ®é›†çœŸå®æ•°æ®ï¼‰" if real_physical_exam else "ï¼ˆLLMç”Ÿæˆï¼‰"),
                chunks=[],
                flags=["REAL_DATA"] if real_physical_exam else (["LLM_PARSE_FALLBACK"] if used_fallback else ["LLM_USED"]),
            )
        )
        logger.info("âœ… S5èŠ‚ç‚¹å®Œæˆ\n")
        return state

    def s6_preliminary_judgment(state: BaseState) -> BaseState:
        """S6: é€šç”¨åˆæ­¥åˆ¤æ–­ä¸å¼€å•èŠ‚ç‚¹"""
        dept = state.dept
        dept_config = DEPT_CONFIG.get(dept, DEPT_CONFIG.get("internal_medicine", {}))
        dept_name = dept_config.get("name", "é€šç”¨")
        alarm_keywords = dept_config.get("alarm_keywords", [])
        common_tests = dept_config.get("common_tests", ["è¡€å¸¸è§„"])
        
        logger.info("\n" + "="*60)
        logger.info(f"ğŸ”¬ S6: {dept_name}åˆæ­¥åˆ¤æ–­")
        logger.info("="*60)
        
        query = f"{dept} {dept_name} æ£€æŸ¥é€‰æ‹© é€‚åº”ç—‡ {state.chief_complaint}"
        logger.info(f"ğŸ” æ£€ç´¢{dept_name}æ£€æŸ¥æŒ‡å—...")
        chunks = retriever.retrieve(query, filters={"dept": dept}, k=4)
        state.add_retrieved_chunks(chunks)
        logger.info(f"  âœ… æ£€ç´¢åˆ° {len(chunks)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

        cc = state.chief_complaint
        
        # ä½¿ç”¨LLMç”Ÿæˆæ£€æŸ¥æ–¹æ¡ˆ
        logger.info("\nğŸ¤– ä½¿ç”¨LLMç”Ÿæˆæ£€æŸ¥æ–¹æ¡ˆ...")
        system_prompt = load_prompt("common_system.txt")
        
        # å°è¯•åŠ è½½ç§‘å®¤ç‰¹å®šprompt
        specialty_prompt_file = f"{dept}_specialty.txt"
        try:
            specialty_prompt = load_prompt(specialty_prompt_file)
        except:
            specialty_prompt = f"è¯·æ ¹æ®{dept_name}ç—‡çŠ¶åˆ¶å®šæ£€æŸ¥æ–¹æ¡ˆã€‚"
        
        # å¼ºåŒ–æç¤ºè¯ï¼šæ˜ç¡®typeæ ‡å‡†ï¼Œå®Œå…¨ç”±LLMåˆ¤æ–­æ£€æŸ¥åˆç†æ€§
        user_prompt = (
            specialty_prompt
            + "\n\nã€ä»»åŠ¡ã€‘æ ¹æ®æ‚£è€…æƒ…å†µï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¾…åŠ©æ£€æŸ¥å¹¶ç»™å‡ºåˆæ­¥è¯„ä¼°ã€‚\n\n"
            + "ã€æŒ‡å¯¼åŸåˆ™ã€‘\n"
            + f"- è­¦æŠ¥ç—‡çŠ¶ï¼š{', '.join(alarm_keywords)}\n"
            + f"- å¸¸è§„æ£€æŸ¥å‚è€ƒï¼š{', '.join(common_tests)}\n"
            + "- ç—‡çŠ¶è½»å¾®ä¸”æ˜ç¡®ï¼šå¯ä¸å¼€æ£€æŸ¥ï¼Œç»™äºˆå»ºè®®\n"
            + "- ç—‡çŠ¶å¤æ‚æˆ–æœ‰è­¦æŠ¥ä¿¡å·ï¼šå¼€å…·å¿…è¦æ£€æŸ¥\n"
            + "- ä½ å®Œå…¨è‡ªä¸»åˆ¤æ–­å“ªäº›æ£€æŸ¥åˆç†ï¼Œä¸å—é™äºåˆ—è¡¨\n\n"
            + "ã€æ‚£è€…ä¿¡æ¯ã€‘\n"
            + json.dumps(
                {
                    "chief_complaint": state.chief_complaint,
                    "history": state.history,
                    "exam_findings": state.exam_findings,
                    f"{dept}_interview": state.dept_payload.get(dept, {}).get("interview", {}),
                },
                ensure_ascii=False,
                indent=2
            )
            + "\n\nã€å‚è€ƒçŸ¥è¯†ã€‘\n" + _chunks_for_prompt(chunks)
            + "\n\nã€è¾“å‡ºè¦æ±‚ã€‘JSONæ ¼å¼ï¼š\n"
            + "1. need_aux_tests (bool): æ˜¯å¦éœ€è¦æ£€æŸ¥\n"
            + "2. ordered_tests (list): æ£€æŸ¥é¡¹ç›®åˆ—è¡¨ï¼Œæ¯é¡¹å¿…é¡»åŒ…å«ï¼š\n"
            + "   - dept: ç§‘å®¤ä»£ç ï¼ˆå¦‚\"internal_medicine\"ï¼‰\n"
            + "   - type: æ£€æŸ¥ç±»å‹ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š\"lab\"ï¼ˆæ£€éªŒï¼‰/\"imaging\"ï¼ˆå½±åƒï¼‰/\"endoscopy\"ï¼ˆå†…é•œï¼‰/\"neurophysiology\"ï¼ˆç”µç”Ÿç†ï¼‰\n"
            + "   - name: æ£€æŸ¥åç§°ï¼ˆå…·ä½“é¡¹ç›®åï¼‰\n"
            + "   - reason: å¼€å…·åŸå› \n"
            + "   - priority: ä¼˜å…ˆçº§ï¼ˆ\"urgent\"ç´§æ€¥/\"routine\"å¸¸è§„ï¼‰\n"
            + "   - need_prep: æ˜¯å¦éœ€è¦å‡†å¤‡ï¼ˆboolï¼‰\n"
            + "   - need_schedule: æ˜¯å¦éœ€è¦é¢„çº¦ï¼ˆboolï¼‰\n"
            + "3. specialty_summary (dict): åŒ…å«problem_list, assessment, plan_direction, red_flags\n\n"
            + "âš ï¸ é‡è¦ï¼štypeå­—æ®µå¿…é¡»ä¸¥æ ¼ä½¿ç”¨æ ‡å‡†å€¼ï¼ˆlab/imaging/endoscopy/neurophysiologyï¼‰ï¼Œä¸è¦ä½¿ç”¨ä¸­æ–‡æˆ–å…¶ä»–æè¿°ï¼"
        )
        
        # æ£€æŸ¥LLMæ˜¯å¦å¯ç”¨
        if llm is None:
            logger.error("âš ï¸  æœªLLMé…ç½®ï¼Œæ— æ³•ç”Ÿæˆæ£€æŸ¥æ–¹æ¡ˆ")
            # ä½¿ç”¨ä¿å®ˆçš„fallback
            obj = {
                "need_aux_tests": False,
                "ordered_tests": [],
                "specialty_summary": {
                    "problem_list": [f"{dept_name}ç—‡çŠ¶å¾…è¯„ä¼°"],
                    "assessment": "LLMä¸å¯ç”¨ï¼Œæ— æ³•ç”Ÿæˆæ£€æŸ¥æ–¹æ¡ˆ",
                    "plan_direction": "éœ€é…ç½®LLM",
                    "red_flags": []
                },
            }
            used_fallback = True
        else:
            # ä¼˜åŒ–fallbackä¸ºä¿å®ˆç­–ç•¥
            obj, used_fallback, _raw = llm.generate_json(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            fallback=lambda: {
                "need_aux_tests": False,  # æ”¹ä¸ºä¿å®ˆç­–ç•¥ï¼šä¸ç¡®å®šæ—¶ä¸å¼€å•
                "ordered_tests": [],
                "specialty_summary": {
                    "problem_list": [f"{dept_name}ç—‡çŠ¶å¾…è¯„ä¼°"],
                    "assessment": "ä¿¡æ¯ä¸è¶³ï¼Œå»ºè®®è¿›ä¸€æ­¥é—®è¯Š",
                    "plan_direction": "å®Œå–„ç—…å²é‡‡é›†",
                    "red_flags": []
                },
            },
            temperature=0.2,
        )
        need_aux_tests = bool(obj.get("need_aux_tests", False))
        ordered = list(obj.get("ordered_tests") or [])
        summary = dict(obj.get("specialty_summary") or {})
        logger.info("  âœ… æ£€æŸ¥æ–¹æ¡ˆç”Ÿæˆå®Œæˆ")

        # æ ‡å‡†åŒ–æ£€æŸ¥é¡¹ç›®ï¼ˆä¸åšç™½åå•è¿‡æ»¤ï¼Œå®Œå…¨ä¿¡ä»»LLMåˆ¤æ–­ï¼‰
        normalized: list[dict[str, Any]] = []
        for t in ordered:
            if not isinstance(t, dict):
                continue
            normalized_test = _validate_and_normalize_test(t, dept, dept_config)
            if normalized_test:
                normalized.append(normalized_test)
            else:
                logger.warning(f"  âš ï¸  æ£€æŸ¥é¡¹ç›® '{t.get('name')}' æ ‡å‡†åŒ–å¤±è´¥ï¼Œè·³è¿‡")
        
        ordered = normalized
        
        # å¦‚æœæ ‡å‡†åŒ–åæ²¡æœ‰é¡¹ç›®ï¼Œæ›´æ–°çŠ¶æ€
        if need_aux_tests and not ordered:
            logger.warning("  âš ï¸  åŸè®¡åˆ’å¼€å•ä½†æ ‡å‡†åŒ–åæ— æœ‰æ•ˆé¡¹ç›®ï¼Œæ”¹ä¸ºä¸å¼€å•")
            need_aux_tests = False
        
        # æ›´æ–°çŠ¶æ€
        state.need_aux_tests = need_aux_tests
        state.ordered_tests = ordered
        state.specialty_summary = summary
        
        decision = "éœ€è¦è¾…åŠ©æ£€æŸ¥ä»¥æ˜ç¡®è¯Šæ–­" if need_aux_tests else "æš‚æ— éœ€è¾…åŠ©æ£€æŸ¥ï¼Œç»™å‡ºå¯¹ç—‡æ–¹å‘"
        
        logger.info(f"\n  ğŸ“‹ å¼€å•å†³ç­–: need_aux_tests={state.need_aux_tests}")
        if ordered:
            logger.info(f"  ğŸ“ å¼€å•é¡¹ç›® ({len(ordered)}é¡¹):")
            for test in ordered:
                logger.info(f"     - {test['name']} ({test['type']}) - {test.get('priority', 'routine')}")

        state.dept_payload.setdefault(dept, {})
        state.dept_payload[dept]["preliminary"] = {
            "need_aux_tests": state.need_aux_tests,
            "ordered_tests_count": len(ordered),
        }

        state.add_audit(
            make_audit_entry(
                node_name=f"S6 {dept_name} Preliminary Judgment",
                inputs_summary={"chief_complaint": state.chief_complaint, "dept": dept},
                outputs_summary={
                    "need_aux_tests": state.need_aux_tests,
                    "ordered_tests": [t["name"] for t in ordered],
                },
                decision=decision,
                chunks=chunks,
                flags=["LLM_PARSE_FALLBACK"] if used_fallback else ["LLM_USED"],
            )
        )
        logger.info("âœ… S6èŠ‚ç‚¹å®Œæˆ\n")
        return state

    # æ„å»ºå›¾ç»“æ„
    graph.add_node("S4", s4_specialty_interview)
    graph.add_node("S5", s5_physical_exam)
    graph.add_node("S6", s6_preliminary_judgment)

    graph.set_entry_point("S4")
    graph.add_edge("S4", "S5")
    graph.add_edge("S5", "S6")
    graph.add_edge("S6", END)
    
    return graph.compile()
