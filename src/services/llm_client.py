from __future__ import annotations

import logging
import os
import time
from dataclasses import dataclass
from typing import Any, Callable, Protocol

import httpx

from utils import parse_json_with_retry, get_logger

logger = get_logger(__name__)

# ç¦ç”¨httpxçš„è¯¦ç»†æ—¥å¿—è¾“å‡ºï¼ˆé¿å…æ˜¾ç¤ºæ¯æ¬¡HTTPè¯·æ±‚ï¼‰
logging.getLogger("httpx").setLevel(logging.WARNING)


class LLMClient(Protocol):
    def generate_json(
        self,
        *,
        system_prompt: str,
        user_prompt: str,
        fallback: Callable[[], dict[str, Any]],
        temperature: float = 0.2,
        max_tokens: int = 1200,
    ) -> tuple[dict[str, Any], bool, str]:
        """Return (json_obj, used_fallback, raw_text)."""
    
    def generate_text(
        self,
        *,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 500,
    ) -> str:
        """Generate plain text response."""


@dataclass(frozen=True)
class DeepSeekConfig:
    api_key: str
    base_url: str
    model: str
    timeout_s: float = 120.0
    max_retries: int = 3
    retry_delay: float = 2.0


class DeepSeekLLMClient:
    """DeepSeek Chat Completions (OpenAI-compatible) client.

    Security:
    - Do NOT hardcode API keys in code. Use env var `DEEPSEEK_API_KEY`.
    - Avoid sending sensitive patient identifiers to any external API.
    """

    def __init__(self, config: DeepSeekConfig) -> None:
        self.config = config

    @staticmethod
    def from_env() -> "DeepSeekLLMClient":
        # ä»ç¯å¢ƒå˜é‡è¯»å–æ‰€æœ‰é…ç½®ï¼ˆæ— é»˜è®¤å€¼ï¼Œç¡®ä¿é…ç½®æ¥è‡ª.envæ–‡ä»¶ï¼‰
        api_key = os.getenv("DEEPSEEK_API_KEY", "").strip()
        if not api_key:
            raise RuntimeError(
                "ç¼ºå°‘ç¯å¢ƒå˜é‡: DEEPSEEK_API_KEY\n"
                "è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DEEPSEEK_API_KEY=your-api-key"
            )
        
        base_url = os.getenv("DEEPSEEK_BASE_URL", "").strip()
        if not base_url:
            raise RuntimeError(
                "ç¼ºå°‘ç¯å¢ƒå˜é‡: DEEPSEEK_BASE_URL\n"
                "è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DEEPSEEK_BASE_URL=https://api.apiyi.com/v1"
            )
        
        model = os.getenv("DEEPSEEK_MODEL", "").strip()
        if not model:
            raise RuntimeError(
                "ç¼ºå°‘ç¯å¢ƒå˜é‡: DEEPSEEK_MODEL\n"
                "è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DEEPSEEK_MODEL=gpt-4o-mini"
            )
        
        timeout_s = float(os.getenv("DEEPSEEK_TIMEOUT_S", "120"))
        max_retries = int(os.getenv("DEEPSEEK_MAX_RETRIES", "3"))
        retry_delay = float(os.getenv("DEEPSEEK_RETRY_DELAY", "2.0"))
        
        # è¾“å‡ºå®é™…ä½¿ç”¨çš„é…ç½®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        logger.info(f"ğŸ”§ LLMé…ç½®: URL={base_url}, Model={model}, Key={api_key[:12]}...")
        
        return DeepSeekLLMClient(
            DeepSeekConfig(
                api_key=api_key, 
                base_url=base_url, 
                model=model, 
                timeout_s=timeout_s,
                max_retries=max_retries,
                retry_delay=retry_delay
            )
        )

    def _chat(self, *, system_prompt: str, user_prompt: str, temperature: float, max_tokens: int, json_mode: bool = True) -> str:
        url = self.config.base_url.rstrip("/") + "/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json",
        }
        payload: dict[str, Any] = {
            "model": self.config.model,
            "temperature": float(temperature),
            "max_tokens": int(max_tokens),
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            "stream": False,  # æ˜ç¡®ç¦ç”¨æµå¼è¾“å‡º
        }
        # Add JSON mode only when requested (æŸäº›APIå¯èƒ½ä¸æ”¯æŒ)
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        # è°ƒè¯•æ—¥å¿—
        logger.debug(f"ğŸ“¡ APIè¯·æ±‚: {url}")
        logger.debug(f"ğŸ”‘ API Key (å‰8ä½): {self.config.api_key[:8]}...")
        logger.debug(f"ğŸ“‹ æ¨¡å‹: {self.config.model}")
        
        # é‡è¯•æœºåˆ¶
        last_exception = None
        for attempt in range(self.config.max_retries):
            try:
                # é…ç½®httpxå®¢æˆ·ç«¯ï¼Œå¢åŠ è¿æ¥æ± å’Œè¶…æ—¶è®¾ç½®
                with httpx.Client(
                    timeout=httpx.Timeout(
                        connect=10.0,  # è¿æ¥è¶…æ—¶
                        read=self.config.timeout_s,  # è¯»å–è¶…æ—¶
                        write=10.0,  # å†™å…¥è¶…æ—¶
                        pool=5.0  # è¿æ¥æ± è¶…æ—¶
                    ),
                    limits=httpx.Limits(
                        max_keepalive_connections=5,
                        max_connections=10
                    )
                ) as client:
                    resp = client.post(url, headers=headers, json=payload)
                    resp.raise_for_status()
                    data = resp.json()
                
                # æˆåŠŸè·å–å“åº”
                try:
                    return str(data["choices"][0]["message"]["content"])
                except Exception as e:  # noqa: BLE001
                    raise RuntimeError(f"Unexpected DeepSeek response shape: {data}") from e
                    
            except (httpx.RemoteProtocolError, httpx.ReadTimeout, httpx.ConnectTimeout, httpx.NetworkError) as e:
                last_exception = e
                
                # åˆ¤æ–­æ˜¯å¦éœ€è¦é‡è¯•
                if attempt < self.config.max_retries - 1:
                    wait_time = self.config.retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                    logger.warning(
                        f"âš ï¸  DeepSeek APIè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{self.config.max_retries}): {e.__class__.__name__}: {str(e)[:100]}"
                    )
                    logger.info(f"   ç­‰å¾… {wait_time:.1f}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•° ({self.config.max_retries})")
                    
            except httpx.HTTPStatusError as e:
                # HTTPçŠ¶æ€é”™è¯¯ï¼ˆå¦‚429, 500ç­‰ï¼‰
                if e.response.status_code == 401:
                    # 401é”™è¯¯ï¼šè®¤è¯å¤±è´¥ï¼Œæä¾›è¯¦ç»†ä¿¡æ¯
                    try:
                        error_detail = e.response.json()
                        logger.error(f"âŒ APIè®¤è¯å¤±è´¥ (401)")
                        logger.error(f"   URL: {url}")
                        logger.error(f"   API Key (å‰8ä½): {self.config.api_key[:8]}...")
                        logger.error(f"   æ¨¡å‹: {self.config.model}")
                        logger.error(f"   é”™è¯¯è¯¦æƒ…: {error_detail}")
                    except Exception:
                        logger.error(f"âŒ APIè®¤è¯å¤±è´¥ (401): {e.response.text[:200]}")
                    
                    raise RuntimeError(
                        f"APIè®¤è¯å¤±è´¥ (401)ã€‚è¯·æ£€æŸ¥:\n"
                        f"1. APIå¯†é’¥æ˜¯å¦æ­£ç¡®: {self.config.api_key[:12]}...\n"
                        f"2. API URLæ˜¯å¦æ­£ç¡®: {self.config.base_url}\n"
                        f"3. å¯†é’¥æ˜¯å¦æœ‰æƒé™è®¿é—®æ¨¡å‹: {self.config.model}"
                    ) from e
                else:
                    logger.error(f"âŒ APIè¿”å›é”™è¯¯çŠ¶æ€: {e.response.status_code}")
                    logger.error(f"   å“åº”å†…å®¹: {e.response.text[:200]}")
                    raise
                
            except Exception as e:
                # å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
                logger.error(f"âŒ DeepSeek APIè°ƒç”¨å‡ºç°æœªçŸ¥é”™è¯¯: {e.__class__.__name__}: {str(e)[:100]}")
                raise
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
        if last_exception:
            raise RuntimeError(
                f"DeepSeek APIè°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯•{self.config.max_retries}æ¬¡ï¼‰: {last_exception.__class__.__name__}: {str(last_exception)}"
            ) from last_exception
        else:
            raise RuntimeError("DeepSeek APIè°ƒç”¨å¤±è´¥ï¼ˆåŸå› æœªçŸ¥ï¼‰")

    def generate_json(
        self,
        *,
        system_prompt: str,
        user_prompt: str,
        fallback: Callable[[], dict[str, Any]],
        temperature: float = 0.2,
        max_tokens: int = 1200,
    ) -> tuple[dict[str, Any], bool, str]:
        raw = self._chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            json_mode=True,
        )
        obj, used_fallback = parse_json_with_retry(raw, fallback=fallback)
        return obj, used_fallback, raw
    
    def generate_text(
        self,
        *,
        system_prompt: str,
        user_prompt: str,
        temperature: float = 0.7,
        max_tokens: int = 500,
    ) -> str:
        """Generate plain text response (not JSON)."""
        return self._chat(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            json_mode=False,
        )


def build_llm_client(mode: str | None) -> LLMClient | None:
    """Factory used by CLI/router. `mode` can be: None/'mock'/'deepseek'."""

    if mode is None or mode == "mock":
        return None
    if mode == "deepseek":
        return DeepSeekLLMClient.from_env()
    raise ValueError(f"Unknown LLM mode: {mode}")

