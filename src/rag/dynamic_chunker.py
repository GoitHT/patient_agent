"""åŠ¨æ€ Chunk ç­–ç•¥ - æ ¹æ®å†…å®¹ç±»å‹è‡ªé€‚åº”åˆ†å—
å®ç°æ™ºèƒ½æ–‡æ¡£åˆ†å—ï¼Œæå‡æ£€ç´¢è´¨é‡
"""
from __future__ import annotations

import re
import logging
from typing import List, Dict, Any
from dataclasses import dataclass
from enum import Enum


class ChunkStrategy(Enum):
    """åˆ†å—ç­–ç•¥ç±»å‹"""
    FIXED = "fixed"  # å›ºå®šå¤§å°
    SEMANTIC = "semantic"  # è¯­ä¹‰åˆ†å—
    HIERARCHICAL = "hierarchical"  # å±‚æ¬¡åˆ†å—
    ADAPTIVE = "adaptive"  # è‡ªé€‚åº”åˆ†å—


@dataclass
class ChunkConfig:
    """åˆ†å—é…ç½®"""
    strategy: ChunkStrategy
    chunk_size: int
    chunk_overlap: int
    min_chunk_size: int = 50
    max_chunk_size: int = 2000
    
    # è¯­ä¹‰åˆ†å—å‚æ•°
    semantic_threshold: float = 0.7  # è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
    
    # å±‚æ¬¡åˆ†å—å‚æ•°
    hierarchy_levels: List[str] = None  # å±‚æ¬¡æ ‡è®°ï¼ˆå¦‚ï¼šç« èŠ‚ã€æ®µè½ï¼‰
    
    # è‡ªé€‚åº”å‚æ•°
    content_density_threshold: float = 0.3  # å†…å®¹å¯†åº¦é˜ˆå€¼ï¼ˆä¸­æ–‡å­—ç¬¦/æ€»å­—ç¬¦ï¼‰


class DynamicChunker:
    """åŠ¨æ€åˆ†å—å™¨ - æ ¹æ®å†…å®¹è‡ªé€‚åº”é€‰æ‹©åˆ†å—ç­–ç•¥
    
    ç‰¹æ€§ï¼š
        - åŒ»å­¦æ–‡æœ¬è¯†åˆ«ï¼ˆæŒ‡å—ã€ç—…ä¾‹ã€é—®ç­”ï¼‰
        - è‡ªé€‚åº”å—å¤§å°ï¼ˆæ ¹æ®å†…å®¹å¯†åº¦ï¼‰
        - ä¿ç•™æ–‡æ¡£ç»“æ„ï¼ˆæ ‡é¢˜ã€åˆ—è¡¨ã€æ®µè½ï¼‰
        - æ™ºèƒ½é‡å ï¼ˆé¿å…æˆªæ–­å…³é”®ä¿¡æ¯ï¼‰
    """
    
    def __init__(
        self,
        default_config: ChunkConfig = None,
        logger: logging.Logger = None
    ):
        """
        Args:
            default_config: é»˜è®¤åˆ†å—é…ç½®
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.default_config = default_config or ChunkConfig(
            strategy=ChunkStrategy.ADAPTIVE,
            chunk_size=500,
            chunk_overlap=50
        )
        self._logger = logger or logging.getLogger("hospital_agent.chunker")
    
    def chunk_documents(
        self,
        documents: List[Dict[str, Any]],
        config: ChunkConfig = None
    ) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†å—æ–‡æ¡£
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨ï¼Œæ ¼å¼: [{"text": "...", "meta": {...}}]
            config: åˆ†å—é…ç½®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        config = config or self.default_config
        chunked_docs = []
        
        for doc in documents:
            text = doc.get("text", "")
            meta = doc.get("meta", {})
            
            # è¯†åˆ«æ–‡æ¡£ç±»å‹
            doc_type = self._identify_document_type(text, meta)
            
            # é€‰æ‹©åˆ†å—ç­–ç•¥
            strategy = self._select_strategy(text, doc_type, config)
            
            # æ‰§è¡Œåˆ†å—
            chunks = self._chunk_text(text, strategy, config)
            
            # æ·»åŠ å…ƒæ•°æ®
            for i, chunk in enumerate(chunks):
                chunked_docs.append({
                    "text": chunk,
                    "meta": {
                        **meta,
                        "chunk_id": i,
                        "total_chunks": len(chunks),
                        "doc_type": doc_type,
                        "chunk_strategy": strategy.value,
                        "chunk_size": len(chunk)
                    }
                })
        
        self._logger.info(
            f"ğŸ“ åˆ†å—å®Œæˆ: {len(documents)} æ–‡æ¡£ â†’ {len(chunked_docs)} å—"
        )
        return chunked_docs
    
    def _identify_document_type(self, text: str, meta: Dict[str, Any]) -> str:
        """è¯†åˆ«æ–‡æ¡£ç±»å‹
        
        Returns:
            æ–‡æ¡£ç±»å‹: "guideline", "case", "qa", "dialogue", "general"
        """
        # ä»å…ƒæ•°æ®è¯†åˆ«
        if "type" in meta:
            return meta["type"]
        
        # ä»å†…å®¹ç‰¹å¾è¯†åˆ«
        text_lower = text.lower()
        
        # åŒ»å­¦æŒ‡å—ç‰¹å¾
        guideline_keywords = ["è¯Šç–—æŒ‡å—", "ä¸´åºŠè·¯å¾„", "å…±è¯†", "æ ‡å‡†æ“ä½œç¨‹åº", "SOP"]
        if any(kw in text for kw in guideline_keywords):
            return "guideline"
        
        # ä¸´åºŠç—…ä¾‹ç‰¹å¾
        case_keywords = ["ä¸»è¯‰", "ç°ç—…å²", "æ—¢å¾€å²", "ä½“æ ¼æ£€æŸ¥", "è¯Šæ–­", "æ²»ç–—æ–¹æ¡ˆ"]
        case_count = sum(1 for kw in case_keywords if kw in text)
        if case_count >= 3:
            return "case"
        
        # é—®ç­”å¯¹ç‰¹å¾
        if "é—®ï¼š" in text or "ç­”ï¼š" in text or "Q:" in text or "A:" in text:
            return "qa"
        
        # å¯¹è¯è®°å½•ç‰¹å¾
        if "åŒ»ç”Ÿï¼š" in text or "æ‚£è€…ï¼š" in text:
            return "dialogue"
        
        return "general"
    
    def _select_strategy(
        self,
        text: str,
        doc_type: str,
        config: ChunkConfig
    ) -> ChunkStrategy:
        """æ ¹æ®æ–‡æ¡£ç±»å‹å’Œå†…å®¹é€‰æ‹©åˆ†å—ç­–ç•¥"""
        # å¦‚æœé…ç½®æŒ‡å®šäº†å›ºå®šç­–ç•¥ï¼Œç›´æ¥ä½¿ç”¨
        if config.strategy != ChunkStrategy.ADAPTIVE:
            return config.strategy
        
        # è‡ªé€‚åº”é€‰æ‹©ç­–ç•¥
        if doc_type == "guideline":
            # åŒ»å­¦æŒ‡å—ï¼šå±‚æ¬¡åˆ†å—ï¼ˆä¿ç•™ç« èŠ‚ç»“æ„ï¼‰
            return ChunkStrategy.HIERARCHICAL
        
        elif doc_type == "case":
            # ä¸´åºŠç—…ä¾‹ï¼šè¯­ä¹‰åˆ†å—ï¼ˆæŒ‰ç—…å†æ®µè½ï¼‰
            return ChunkStrategy.SEMANTIC
        
        elif doc_type == "qa":
            # é—®ç­”å¯¹ï¼šå›ºå®šåˆ†å—ï¼ˆä¿æŒå®Œæ•´æ€§ï¼‰
            return ChunkStrategy.FIXED
        
        elif doc_type == "dialogue":
            # å¯¹è¯è®°å½•ï¼šè¯­ä¹‰åˆ†å—ï¼ˆæŒ‰å¯¹è¯å›åˆï¼‰
            return ChunkStrategy.SEMANTIC
        
        else:
            # ä¸€èˆ¬æ–‡æœ¬ï¼šæ ¹æ®å†…å®¹å¯†åº¦å†³å®š
            density = self._calculate_content_density(text)
            if density > config.content_density_threshold:
                return ChunkStrategy.SEMANTIC
            else:
                return ChunkStrategy.FIXED
    
    def _calculate_content_density(self, text: str) -> float:
        """è®¡ç®—å†…å®¹å¯†åº¦ï¼ˆä¸­æ–‡å­—ç¬¦å æ¯”ï¼‰"""
        if not text:
            return 0.0
        
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        total_chars = len(text)
        
        return chinese_chars / total_chars if total_chars > 0 else 0.0
    
    def _chunk_text(
        self,
        text: str,
        strategy: ChunkStrategy,
        config: ChunkConfig
    ) -> List[str]:
        """æ‰§è¡Œåˆ†å—"""
        if strategy == ChunkStrategy.FIXED:
            return self._fixed_chunk(text, config)
        
        elif strategy == ChunkStrategy.SEMANTIC:
            return self._semantic_chunk(text, config)
        
        elif strategy == ChunkStrategy.HIERARCHICAL:
            return self._hierarchical_chunk(text, config)
        
        else:
            return self._fixed_chunk(text, config)
    
    def _fixed_chunk(self, text: str, config: ChunkConfig) -> List[str]:
        """å›ºå®šå¤§å°åˆ†å—ï¼ˆå¸¦æ™ºèƒ½é‡å ï¼‰"""
        chunks = []
        chunk_size = config.chunk_size
        overlap = config.chunk_overlap
        
        # æŒ‰å¥å­åˆ†å‰²ï¼ˆé¿å…æˆªæ–­å¥å­ï¼‰
        sentences = self._split_sentences(text)
        
        current_chunk = ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # æ·»åŠ é‡å ï¼ˆå–å½“å‰å—çš„æœ€åéƒ¨åˆ†ï¼‰
                overlap_text = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk
                current_chunk = overlap_text + sentence
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _semantic_chunk(self, text: str, config: ChunkConfig) -> List[str]:
        """è¯­ä¹‰åˆ†å—ï¼ˆæŒ‰æ®µè½å’Œè¯­ä¹‰è¾¹ç•Œï¼‰"""
        chunks = []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text.split('\n\n')
        
        current_chunk = ""
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡æœ€å¤§å—å¤§å°
            if len(current_chunk) + len(para) <= config.max_chunk_size:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœå•ä¸ªæ®µè½è¶…è¿‡æœ€å¤§å—å¤§å°ï¼ŒæŒ‰å¥å­åˆ†å‰²
                if len(para) > config.max_chunk_size:
                    sub_chunks = self._fixed_chunk(para, config)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = para + "\n\n"
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _hierarchical_chunk(self, text: str, config: ChunkConfig) -> List[str]:
        """å±‚æ¬¡åˆ†å—ï¼ˆä¿ç•™æ ‡é¢˜å’Œç« èŠ‚ç»“æ„ï¼‰"""
        chunks = []
        
        # è¯†åˆ«æ ‡é¢˜ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
        # æ ¼å¼1: # æ ‡é¢˜, ## æ ‡é¢˜
        # æ ¼å¼2: ä¸€ã€æ ‡é¢˜, 1. æ ‡é¢˜, ï¼ˆä¸€ï¼‰æ ‡é¢˜
        # æ ¼å¼3: ã€æ ‡é¢˜ã€‘
        
        lines = text.split('\n')
        current_section = ""
        current_header = ""
        
        for line in lines:
            line = line.strip()
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜
            is_header = self._is_header(line)
            
            if is_header:
                # ä¿å­˜ä¹‹å‰çš„ç« èŠ‚
                if current_section:
                    chunks.append(f"{current_header}\n{current_section}".strip())
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_header = line
                current_section = ""
            else:
                current_section += line + "\n"
                
                # å¦‚æœå½“å‰ç« èŠ‚è¿‡é•¿ï¼Œåˆ†å—
                if len(current_section) > config.chunk_size:
                    chunk_text = f"{current_header}\n{current_section}".strip()
                    sub_chunks = self._semantic_chunk(chunk_text, config)
                    chunks.extend(sub_chunks)
                    current_section = ""
        
        # æ·»åŠ æœ€åä¸€ä¸ªç« èŠ‚
        if current_section:
            chunks.append(f"{current_header}\n{current_section}".strip())
        
        return chunks if chunks else [text]
    
    def _is_header(self, line: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜è¡Œ"""
        if not line:
            return False
        
        # Markdown æ ‡é¢˜
        if re.match(r'^#{1,6}\s+', line):
            return True
        
        # ä¸­æ–‡ç¼–å·æ ‡é¢˜
        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.]', line):
            return True
        
        if re.match(r'^\d+[ã€.]', line):
            return True
        
        if re.match(r'^[ï¼ˆ\(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰\)]', line):
            return True
        
        # ã€æ ‡é¢˜ã€‘æ ¼å¼
        if re.match(r'^ã€.+ã€‘$', line):
            return True
        
        # å…¨å¤§å†™æˆ–åŠ ç²—æ ‡è®°
        if line.isupper() or line.startswith('**'):
            return True
        
        return False
    
    def _split_sentences(self, text: str) -> List[str]:
        """æ™ºèƒ½å¥å­åˆ†å‰²ï¼ˆä¿ç•™æ ‡ç‚¹ï¼‰"""
        # ä¸­è‹±æ–‡å¥å­ç»“æŸç¬¦
        sentence_endings = r'[ã€‚ï¼ï¼Ÿï¼›.!?;]'
        
        # åˆ†å‰²å¥å­ï¼ˆä¿ç•™åˆ†éš”ç¬¦ï¼‰
        sentences = re.split(f'({sentence_endings})', text)
        
        # é‡ç»„å¥å­ï¼ˆå°†åˆ†éš”ç¬¦é™„åŠ åˆ°å¥å­åï¼‰
        result = []
        for i in range(0, len(sentences) - 1, 2):
            if i + 1 < len(sentences):
                result.append(sentences[i] + sentences[i + 1])
            else:
                result.append(sentences[i])
        
        return [s for s in result if s.strip()]


# å·¥å…·å‡½æ•°
def create_chunker_for_medical_documents() -> DynamicChunker:
    """åˆ›å»ºé€‚ç”¨äºåŒ»å­¦æ–‡æ¡£çš„åˆ†å—å™¨"""
    config = ChunkConfig(
        strategy=ChunkStrategy.ADAPTIVE,
        chunk_size=600,  # åŒ»å­¦æ–‡æœ¬é€‚ä¸­å¤§å°
        chunk_overlap=100,  # è¾ƒå¤§é‡å ä¿è¯ä¸Šä¸‹æ–‡
        min_chunk_size=100,
        max_chunk_size=1500,
        semantic_threshold=0.7,
        content_density_threshold=0.5  # ä¸­æ–‡å†…å®¹ä¸ºä¸»
    )
    
    return DynamicChunker(default_config=config)


__all__ = ["DynamicChunker", "ChunkStrategy", "ChunkConfig", "create_chunker_for_medical_documents"]
