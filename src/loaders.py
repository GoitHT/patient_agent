"""æ•°æ®åŠ è½½å™¨æ¨¡å— - ä» HuggingFace åŠ è½½ DiagnosisArena æ•°æ®é›†"""
from __future__ import annotations

import json
import os
import threading
from typing import Any, Optional

from utils import get_logger

# åˆå§‹åŒ–logger
logger = get_logger("hospital_agent.dataset_loader")

# æ˜¯å¦å¯ç”¨è‡ªåŠ¨ç¿»è¯‘ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
ENABLE_TRANSLATION = os.getenv("ENABLE_DATASET_TRANSLATION", "true").lower() in ("true", "1", "yes")

# å…¨å±€æ•°æ®é›†ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
_DATASET_CACHE: dict[str, Any] = {}
_CACHE_ENABLED = True  # æ˜¯å¦å¯ç”¨å†…å­˜ç¼“å­˜
_CACHE_LOCK = threading.RLock()  # ç¼“å­˜é”ï¼Œé˜²æ­¢å¹¶å‘åŠ è½½


def _translate_to_chinese(text: str, field_name: str = "") -> str:
    """
    ä½¿ç”¨LLMå°†è‹±æ–‡åŒ»ç–—æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡
    
    Args:
        text: å¾…ç¿»è¯‘æ–‡æœ¬
        field_name: å­—æ®µåç§°ï¼ˆç”¨äºæç¤ºï¼‰
    
    Returns:
        ç¿»è¯‘åçš„ä¸­æ–‡æ–‡æœ¬
    """
    if not text or not text.strip():
        return text
    
    # å¿«é€Ÿæ£€æµ‹ï¼šå¦‚æœå·²ç»ä¸»è¦æ˜¯ä¸­æ–‡ï¼Œè·³è¿‡ç¿»è¯‘
    chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    if chinese_chars > len(text) * 0.3:  # 30%ä»¥ä¸Šæ˜¯ä¸­æ–‡
        logger.debug(f"  âœ“ {field_name} å·²ä¸ºä¸­æ–‡ï¼Œè·³è¿‡ç¿»è¯‘")
        return text
    
    try:
        from services.llm_client import build_llm_client
        
        llm = build_llm_client("deepseek")
        
        system_prompt = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»ç–—ç¿»è¯‘ä¸“å®¶ï¼Œæ“…é•¿å°†è‹±æ–‡åŒ»ç–—æ–‡æœ¬å‡†ç¡®ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚"
        
        user_prompt = (
            f"è¯·å°†ä»¥ä¸‹åŒ»ç–—æ–‡æœ¬ç¿»è¯‘ä¸ºä¸­æ–‡ã€‚è¦æ±‚ï¼š\n"
            f"1. ä¿æŒåŒ»å­¦æœ¯è¯­çš„å‡†ç¡®æ€§\n"
            f"2. ä¿ç•™æ‰€æœ‰æ•°å€¼ã€å•ä½ã€æ—¶é—´ç­‰å…³é”®ä¿¡æ¯\n"
            f"3. è¯­å¥é€šé¡ºè‡ªç„¶ï¼Œç¬¦åˆä¸­æ–‡åŒ»å­¦è¡¨è¾¾ä¹ æƒ¯\n"
            f"4. ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–é¢å¤–å†…å®¹\n\n"
            f"ã€å¾…ç¿»è¯‘æ–‡æœ¬ã€‘\n{text}\n\n"
            f"ã€ç¿»è¯‘ç»“æœã€‘ï¼ˆä»…è¾“å‡ºç¿»è¯‘åçš„ä¸­æ–‡ï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼‰"
        )
        
        translated = llm.generate_text(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯ç¿»è¯‘å‡†ç¡®æ€§
            max_tokens=1500
        )
        
        # æ¸…ç†å¯èƒ½çš„å‰ç¼€
        translated = translated.strip()
        for prefix in ["ç¿»è¯‘ç»“æœï¼š", "ç¿»è¯‘ï¼š", "ä¸­æ–‡ï¼š", "ã€ç¿»è¯‘ç»“æœã€‘", "ç¿»è¯‘åçš„ä¸­æ–‡ï¼š"]:
            if translated.startswith(prefix):
                translated = translated[len(prefix):].strip()
        
        logger.debug(f"  âœ“ {field_name} ç¿»è¯‘å®Œæˆ ({len(text)} â†’ {len(translated)} å­—ç¬¦)")
        return translated
        
    except Exception as e:
        logger.warning(f"  âš ï¸ {field_name} ç¿»è¯‘å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæ–‡")
        return text


def _translate_case_data(case_data: dict[str, Any]) -> dict[str, Any]:
    """
    å°†ç—…ä¾‹æ•°æ®ç¿»è¯‘ä¸ºä¸­æ–‡
    
    Args:
        case_data: åŸå§‹ç—…ä¾‹æ•°æ®
    
    Returns:
        ç¿»è¯‘åçš„ç—…ä¾‹æ•°æ®
    """
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨ç¿»è¯‘
    if not ENABLE_TRANSLATION:
        logger.info("  â„¹ï¸  è‡ªåŠ¨ç¿»è¯‘å·²ç¦ç”¨ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
        return case_data
    
    logger.info("  ğŸŒ å¼€å§‹ç¿»è¯‘ç—…ä¾‹æ•°æ®ä¸ºä¸­æ–‡...")
    
    translated = {}
    
    # éœ€è¦ç¿»è¯‘çš„å­—æ®µ
    text_fields = [
        "Case Information",
        "Physical Examination", 
        "Diagnostic Tests",
        "Final Diagnosis"
    ]
    
    for field in text_fields:
        if field in case_data:
            original_text = case_data[field]
            translated_text = _translate_to_chinese(original_text, field)
            translated[field] = translated_text
        else:
            translated[field] = ""
    
    # Options éœ€è¦ç‰¹æ®Šå¤„ç†ï¼ˆç¿»è¯‘æ¯ä¸ªé€‰é¡¹ï¼‰
    if "Options" in case_data and isinstance(case_data["Options"], dict):
        logger.debug("  ğŸ”„ ç¿»è¯‘è¯Šæ–­é€‰é¡¹...")
        translated_options = {}
        for key, value in case_data["Options"].items():
            translated_options[key] = _translate_to_chinese(value, f"Option {key}")
        translated["Options"] = translated_options
    else:
        translated["Options"] = {}
    
    # å…¶ä»–å­—æ®µç›´æ¥å¤åˆ¶
    translated["id"] = case_data.get("id", 0)
    translated["Right Option"] = case_data.get("Right Option", "")
    
    logger.info("  âœ… ç—…ä¾‹æ•°æ®ç¿»è¯‘å®Œæˆ")
    return translated


def load_diagnosis_arena_case(case_id: int | None = None, use_mock: bool = False, local_cache_dir: str = "./diagnosis_dataset") -> dict[str, Any]:
    """
    ä» HuggingFace åŠ è½½è¯Šæ–­æ•°æ®é›†ï¼ˆæ”¯æŒæœ¬åœ°ç¼“å­˜ï¼‰
    
    Args:
        case_id: ç—…ä¾‹IDï¼ŒNoneè¡¨ç¤ºéšæœº
        use_mock: æ˜¯å¦ç›´æ¥ä½¿ç”¨Mockæ•°æ®ï¼ˆè·³è¿‡HuggingFaceåŠ è½½ï¼‰
        local_cache_dir: æœ¬åœ°ç¼“å­˜ç›®å½•ï¼ˆé¦–æ¬¡ä»HFä¸‹è½½åä¿å­˜åˆ°æ­¤ç›®å½•ï¼‰
    
    æ•°æ®æ ¼å¼ï¼š
    {
        "id": 1,
        "Case Information": "æ‚£è€…åŸºæœ¬ä¿¡æ¯+ä¸»è¯‰",
        "Physical Examination": "ä½“æ ¼æ£€æŸ¥ç»“æœ",
        "Diagnostic Tests": "å®éªŒå®¤/å½±åƒæ£€æŸ¥ç»“æœ",
        "Final Diagnosis": "æœ€ç»ˆè¯Šæ–­ï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰",
        "Options": {"A": "...", "B": "...", "C": "...", "D": "..."},
        "Right Option": "A"
    }
    
    Returns:
        {
            "full_case": dict,  # å®Œæ•´ç—…ä¾‹ï¼ˆå«æ ‡å‡†ç­”æ¡ˆï¼‰
            "known_case": dict,  # æ‚£è€…å¯è§éƒ¨åˆ†ï¼ˆä»… Case Informationï¼‰
            "ground_truth": dict  # æ ‡å‡†ç­”æ¡ˆï¼ˆFinal Diagnosis, Right Optionï¼‰
        }
    """
    # å¦‚æœæŒ‡å®šä½¿ç”¨Mockæ•°æ®ï¼Œç›´æ¥è¿”å›
    if use_mock:
        return _get_mock_case(case_id)
    
    try:
        from datasets import load_dataset
        from pathlib import Path
        
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦å­˜åœ¨
        cache_path = Path(local_cache_dir)
        local_json = cache_path / "dataset.json"
        
        # æ„å»ºç¼“å­˜é”®ï¼ˆåŸºäºæ–‡ä»¶è·¯å¾„ï¼‰
        cache_key = str(local_json.absolute())
        
        # ä½¿ç”¨é”ä¿æŠ¤ç¼“å­˜æ£€æŸ¥å’ŒåŠ è½½è¿‡ç¨‹
        with _CACHE_LOCK:
            # æ£€æŸ¥å†…å­˜ç¼“å­˜
            if _CACHE_ENABLED and cache_key in _DATASET_CACHE:
                dataset = _DATASET_CACHE[cache_key]
                # é™é»˜ä½¿ç”¨ç¼“å­˜ï¼Œä¸è¾“å‡ºæ—¥å¿—ï¼ˆé¿å…é‡å¤æ—¥å¿—ï¼‰
            else:
                # ä¼˜å…ˆä»æœ¬åœ°åŠ è½½
                if local_json.exists():
                    # äºŒæ¬¡æ£€æŸ¥ï¼šå¯èƒ½å…¶ä»–çº¿ç¨‹å·²ç»åŠ è½½äº†
                    if _CACHE_ENABLED and cache_key in _DATASET_CACHE:
                        dataset = _DATASET_CACHE[cache_key]
                    else:
                        logger.info(f"ğŸ“‚ ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ•°æ®é›†: {local_json}")
                        try:
                            dataset = load_dataset("json", data_files=str(local_json), split="train")
                            logger.info(f"âœ… æœ¬åœ°æ•°æ®é›†åŠ è½½æˆåŠŸ (å…± {len(dataset)} æ¡)")
                            
                            # å­˜å…¥å†…å­˜ç¼“å­˜
                            if _CACHE_ENABLED:
                                _DATASET_CACHE[cache_key] = dataset
                        except Exception as e:
                            logger.warning(f"âš ï¸ æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œå°è¯•ä» HuggingFace é‡æ–°ä¸‹è½½")
                            dataset = None
                else:
                    dataset = None
            
            # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä» HuggingFace ä¸‹è½½
            if dataset is None:
                # ä¸‰æ¬¡æ£€æŸ¥ï¼šå¯èƒ½å…¶ä»–çº¿ç¨‹åˆšåˆšä¸‹è½½å®Œæˆ
                if _CACHE_ENABLED and cache_key in _DATASET_CACHE:
                    dataset = _DATASET_CACHE[cache_key]
                else:
                    logger.info("ğŸŒ ä» HuggingFace ä¸‹è½½æ•°æ®é›†...")
                    try:
                        dataset = load_dataset("SII-SPIRAL-MED/DiagnosisArena", split="train")
                    except (ValueError, KeyError):
                        # å¦‚æœæ²¡æœ‰train splitï¼Œå°è¯•åŠ è½½æ•´ä¸ªæ•°æ®é›†
                        dataset = load_dataset("SII-SPIRAL-MED/DiagnosisArena")
                        # å–ç¬¬ä¸€ä¸ªsplit
                        if isinstance(dataset, dict):
                            split_name = list(dataset.keys())[0]
                            dataset = dataset[split_name]
                    
                    # ä¿å­˜åˆ°æœ¬åœ°
                    logger.info(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°æœ¬åœ°: {local_json}")
                    cache_path.mkdir(parents=True, exist_ok=True)
                    # ä½¿ç”¨ orient='records' æ ¼å¼ä¿å­˜ï¼Œé¿å… Arrow æ ¼å¼é—®é¢˜
                    dataset.to_json(str(local_json), force_ascii=False, orient='records', lines=True)
                    logger.info(f"âœ… æ•°æ®é›†å·²ä¿å­˜ (å…± {len(dataset)} æ¡)")
                    
                    # å­˜å…¥å†…å­˜ç¼“å­˜
                    if _CACHE_ENABLED:
                        _DATASET_CACHE[cache_key] = dataset
        
        # å¦‚æœæŒ‡å®š case_idï¼Œè·å–ç‰¹å®šç—…ä¾‹
        if case_id is not None:
            if case_id < 0 or case_id >= len(dataset):
                raise ValueError(f"case_id {case_id} è¶…å‡ºèŒƒå›´ [0, {len(dataset)-1}]")
            case_data = dataset[case_id]
        else:
            # éšæœºé€‰æ‹©ä¸€ä¸ªç—…ä¾‹
            import random
            case_data = dataset[random.randint(0, len(dataset) - 1)]
        
        # è§£ææ•°æ®
        full_case = {
            "id": case_data.get("id", 0),
            "Case Information": case_data.get("Case Information", ""),
            "Physical Examination": case_data.get("Physical Examination", ""),
            "Diagnostic Tests": case_data.get("Diagnostic Tests", ""),
            "Final Diagnosis": case_data.get("Final Diagnosis", ""),
            "Options": case_data.get("Options", {}),
            "Right Option": case_data.get("Right Option", ""),
        }
        
        # ç¿»è¯‘ä¸ºä¸­æ–‡
        logger.info(f"ğŸ“š åŠ è½½ç—…ä¾‹ID: {full_case['id']}")
        full_case = _translate_case_data(full_case)
        
        # æ‚£è€…å¯è§éƒ¨åˆ†ï¼ˆæ¨¡æ‹ŸçœŸå®æ‚£è€…åªçŸ¥é“è‡ªå·±çš„ç—‡çŠ¶ï¼‰
        known_case = {
            "id": full_case["id"],
            "Case Information": full_case["Case Information"],
            # æ‚£è€…ä¸çŸ¥é“æ£€æŸ¥ç»“æœå’Œè¯Šæ–­
        }
        
        # æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼‰
        ground_truth = {
            "Final Diagnosis": full_case["Final Diagnosis"],
            "Options": full_case["Options"],
            "Right Option": full_case["Right Option"],
            "Physical Examination": full_case["Physical Examination"],
            "Diagnostic Tests": full_case["Diagnostic Tests"],
        }
        
        return {
            "full_case": full_case,
            "known_case": known_case,
            "ground_truth": ground_truth,
        }
        
    except ImportError:
        # å¦‚æœæ²¡æœ‰å®‰è£… datasets åº“ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®
        warning_msg = "âš ï¸ è­¦å‘Šï¼šæœªå®‰è£… datasets åº“ï¼Œä½¿ç”¨Mockç¤ºä¾‹æ•°æ®ï¼çœŸå®æ•°æ®è¯·è¿è¡Œ: pip install datasets"
        print(f"\n{'='*80}")
        print(f"âŒ {warning_msg}")
        print(f"{'='*80}\n")
        logger.warning(warning_msg)
        return _get_mock_case(case_id)
    except Exception as e:
        # å¦‚æœåŠ è½½å¤±è´¥ï¼ˆç½‘ç»œé—®é¢˜ã€æ•°æ®é›†ä¸å­˜åœ¨ç­‰ï¼‰ï¼Œè¿”å›ç¤ºä¾‹æ•°æ®
        error_msg = f"âš ï¸ è­¦å‘Šï¼šæ— æ³•ä» HuggingFace åŠ è½½æ•°æ® ({e})ï¼Œä½¿ç”¨Mockç¤ºä¾‹æ•°æ®ï¼"
        print(f"\n{'='*80}")
        print(f"âŒ {error_msg}")
        print("ğŸ’¡ å¯èƒ½åŸå› :")
        print("   1. ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œæ— æ³•è®¿é—® HuggingFace")
        print("   2. æ•°æ®é›†ä¸å­˜åœ¨æˆ–å·²ç§»é™¤")
        print("   3. HuggingFace token é…ç½®é—®é¢˜")
        print(f"{'='*80}\n")
        logger.error(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return _get_mock_case(case_id)


def _get_mock_case(case_id: int | None = None) -> dict[str, Any]:
    """è¿”å›æ¨¡æ‹Ÿç—…ä¾‹æ•°æ®ï¼ˆå½“æ— æ³•è®¿é—® HuggingFace æ—¶ï¼‰"""
    mock_cases = [
        {
            "id": 1,
            "Case Information": "æ‚£è€…ï¼Œç”·ï¼Œ45å²ï¼Œä¸»è¯‰ï¼šä¸Šè…¹ç—›3å¤©ï¼Œä¼´åé…¸ã€çƒ§å¿ƒã€‚æ—¢å¾€æœ‰å¸çƒŸå²10å¹´ã€‚",
            "Physical Examination": "ä¸Šè…¹éƒ¨è½»å‹ç—›ï¼Œæ— åè·³ç—›ï¼Œå¢¨è²æ°å¾é˜´æ€§ã€‚",
            "Diagnostic Tests": "èƒƒé•œï¼šèƒƒçª¦éƒ¨ç³œçƒ‚æ€§èƒƒç‚ï¼ŒHpé˜³æ€§ã€‚è¡€å¸¸è§„æ­£å¸¸ã€‚",
            "Final Diagnosis": "å¹½é—¨èºæ†èŒç›¸å…³æ€§èƒƒç‚",
            "Options": {
                "A": "å¹½é—¨èºæ†èŒç›¸å…³æ€§èƒƒç‚",
                "B": "èƒƒé£Ÿç®¡åæµç—…",
                "C": "æ¶ˆåŒ–æ€§æºƒç–¡",
                "D": "æ€¥æ€§èƒ°è…ºç‚"
            },
            "Right Option": "A"
        },
        {
            "id": 2,
            "Case Information": "æ‚£è€…ï¼Œå¥³ï¼Œ62å²ï¼Œä¸»è¯‰ï¼šçªå‘å³ä¾§è‚¢ä½“æ— åŠ›ä¼´è¨€è¯­ä¸æ¸…1å°æ—¶ã€‚æœ‰é«˜è¡€å‹ç—…å²5å¹´ã€‚",
            "Physical Examination": "ç¥å¿—æ¸…æ¥šï¼Œå³ä¾§è‚¢ä½“è‚ŒåŠ›3çº§ï¼Œå·´å®¾æ–¯åŸºå¾é˜³æ€§ã€‚",
            "Diagnostic Tests": "å¤´é¢…CTï¼šå·¦ä¾§åŸºåº•èŠ‚åŒºä½å¯†åº¦å½±ã€‚è¡€å‹180/110mmHgã€‚",
            "Final Diagnosis": "æ€¥æ€§è„‘æ¢—æ­»",
            "Options": {
                "A": "è„‘å‡ºè¡€",
                "B": "æ€¥æ€§è„‘æ¢—æ­»",
                "C": "çŸ­æš‚æ€§è„‘ç¼ºè¡€å‘ä½œ",
                "D": "è„‘è‚¿ç˜¤"
            },
            "Right Option": "B"
        },
    ]
    
    idx = case_id if case_id is not None and 0 <= case_id < len(mock_cases) else 0
    case_data = mock_cases[idx]
    
    known_case = {
        "id": case_data["id"],
        "Case Information": case_data["Case Information"],
    }
    
    ground_truth = {
        "Final Diagnosis": case_data["Final Diagnosis"],
        "Options": case_data["Options"],
        "Right Option": case_data["Right Option"],
        "Physical Examination": case_data["Physical Examination"],
        "Diagnostic Tests": case_data["Diagnostic Tests"],
    }
    
    return {
        "full_case": case_data,
        "known_case": known_case,
        "ground_truth": ground_truth,
    }


def clear_dataset_cache():
    """æ¸…é™¤å†…å­˜ä¸­çš„æ•°æ®é›†ç¼“å­˜"""
    global _DATASET_CACHE
    _DATASET_CACHE.clear()
    logger.info("ğŸ—‘ï¸ æ•°æ®é›†å†…å­˜ç¼“å­˜å·²æ¸…é™¤")


def get_cache_info() -> dict[str, Any]:
    """è·å–ç¼“å­˜ä¿¡æ¯"""
    return {
        "enabled": _CACHE_ENABLED,
        "cached_datasets": list(_DATASET_CACHE.keys()),
        "cache_size": len(_DATASET_CACHE),
    }


def _get_dataset_size(local_cache_dir: str = "./diagnosis_dataset") -> int:
    """
    è·å–æ•°æ®é›†å¤§å°ï¼ˆç—…ä¾‹æ•°é‡ï¼‰
    
    Args:
        local_cache_dir: æœ¬åœ°ç¼“å­˜ç›®å½•
    
    Returns:
        æ•°æ®é›†ä¸­çš„ç—…ä¾‹æ•°é‡
    """
    try:
        from datasets import load_dataset
        from pathlib import Path
        
        # æ£€æŸ¥æœ¬åœ°ç¼“å­˜
        cache_path = Path(local_cache_dir)
        local_json = cache_path / "dataset.json"
        cache_key = str(local_json.absolute())
        
        with _CACHE_LOCK:
            # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            if _CACHE_ENABLED and cache_key in _DATASET_CACHE:
                return len(_DATASET_CACHE[cache_key])
            
            if local_json.exists():
                # ä»æœ¬åœ°åŠ è½½ï¼ˆä½¿ç”¨ jsonlines æ ¼å¼ï¼‰
                dataset = load_dataset("json", data_files=str(local_json), split="train")
                # å­˜å…¥ç¼“å­˜
                if _CACHE_ENABLED:
                    _DATASET_CACHE[cache_key] = dataset
                return len(dataset)
            else:
                # ä» HuggingFace åŠ è½½
                try:
                    dataset = load_dataset("SII-SPIRAL-MED/DiagnosisArena", split="train")
                except (ValueError, KeyError):
                    dataset = load_dataset("SII-SPIRAL-MED/DiagnosisArena")
                    if isinstance(dataset, dict):
                        split_name = list(dataset.keys())[0]
                        dataset = dataset[split_name]
                
                # å­˜å…¥ç¼“å­˜
                if _CACHE_ENABLED:
                    _DATASET_CACHE[cache_key] = dataset
                return len(dataset)
    except Exception as e:
        logger.warning(f"è·å–æ•°æ®é›†å¤§å°å¤±è´¥: {e}")
        return 100  # é»˜è®¤å€¼


__all__ = ["load_diagnosis_arena_case", "clear_dataset_cache", "get_cache_info", "_get_dataset_size"]
